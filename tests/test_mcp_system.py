#!/usr/bin/env python3
"""
MCP ì„œë²„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import logging
import os
import uuid
from pathlib import Path
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_mcp_system():
    """MCP ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # ì„œë¹„ìŠ¤ import
        from services.vector_service import VectorService
        from services.prompt_enhancement_service import PromptEnhancementService
        from services.file_indexing_service import FileIndexingService
        from models.prompt_models import PromptEnhanceRequest
        
        logger.info("ğŸš€ MCP ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("1. ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        vector_service = VectorService()
        enhancement_service = PromptEnhancementService(vector_service)
        file_indexing_service = FileIndexingService(vector_service)
        
        # 2. í˜„ì¬ í”„ë¡œì íŠ¸ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸
        logger.info("2. í˜„ì¬ í”„ë¡œì íŠ¸ íŒŒì¼ ì¸ë±ì‹± í…ŒìŠ¤íŠ¸...")
        current_project_path = os.getcwd()
        project_id = "mcp-server-test"
        
        result = await file_indexing_service.index_project_files(
            current_project_path, 
            project_id
        )
        
        if result["success"]:
            logger.info(f"âœ… ì¸ë±ì‹± ì„±ê³µ: {result['indexed_files_count']}ê°œ íŒŒì¼")
            logger.info(f"   í”„ë¡œì íŠ¸ëª…: {result['project_name']}")
            logger.info(f"   ê¸°ìˆ  ìŠ¤íƒ: {result['tech_stack']}")
            logger.info(f"   íŒŒì¼ íŒ¨í„´: {result['file_patterns']}")
        else:
            logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return
        
        # 3. í”„ë¡¬í”„íŠ¸ ê°œì„  í…ŒìŠ¤íŠ¸
        logger.info("3. í”„ë¡¬í”„íŠ¸ ê°œì„  í…ŒìŠ¤íŠ¸...")
        
        test_prompts = [
            "Python FastAPI ì„œë²„ì—ì„œ ë¹„ë™ê¸° API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤˜",
            "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê³  ì‹¶ì–´",
            "MCP ì„œë²„ì— ìƒˆë¡œìš´ ë„êµ¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜",
            "í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ê°œì„ í•´ì¤˜"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            logger.info(f"3.{i} í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {prompt}")
            
            request = PromptEnhanceRequest(
                original_prompt=prompt,
                project_id=project_id,
                context_limit=3
            )
            
            enhanced_result = await enhancement_service.enhance_prompt(request)
            
            logger.info(f"   ì‹ ë¢°ë„ ì ìˆ˜: {enhanced_result.confidence_score:.2f}")
            logger.info(f"   ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸: {len(enhanced_result.context_used)}ê°œ")
            logger.info(f"   ì œì•ˆì‚¬í•­: {len(enhanced_result.suggestions)}ê°œ")
            
            if enhanced_result.enhanced_prompt != request.original_prompt:
                logger.info("   âœ… í”„ë¡¬í”„íŠ¸ê°€ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤")
            else:
                logger.info("   âš ï¸ í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ì œí•œì ì…ë‹ˆë‹¤")
            
            # í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ì— ì €ì¥
            from models.prompt_models import PromptHistory, PromptType
            
            await vector_service.store_prompt_history(
                PromptHistory(
                    id=str(uuid.uuid4()),
                    project_id=project_id,
                    content=prompt,
                    prompt_type=PromptType.USER_QUERY,
                    metadata={"test": True},
                    created_at=datetime.now()
                )
            )
        
        # 4. íŒŒì¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        logger.info("4. íŒŒì¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        
        search_queries = [
            "FastAPI ì„œë²„ ì„¤ì •",
            "ë²¡í„° ì„œë¹„ìŠ¤ êµ¬í˜„",
            "í”„ë¡¬í”„íŠ¸ ê°œì„  ë¡œì§",
            "MCP ë„êµ¬ ì •ì˜"
        ]
        
        for query in search_queries:
            results = await vector_service.search_similar_prompts(
                query=query,
                project_id=project_id,
                limit=3
            )
            
            # íŒŒì¼ ë‚´ìš©ë§Œ í•„í„°ë§
            file_results = [
                r for r in results 
                if r.get('metadata', {}).get('is_file_content', False)
            ]
            
            logger.info(f"   ê²€ìƒ‰ì–´ '{query}': {len(file_results)}ê°œ íŒŒì¼ ê²°ê³¼")
        
        # 5. í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ í…ŒìŠ¤íŠ¸
        logger.info("5. í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ í…ŒìŠ¤íŠ¸...")
        context = await vector_service.get_project_context(project_id)
        
        if context:
            logger.info("   âœ… í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì„±ê³µ")
            metadata = context.get("metadata", {})
            logger.info(f"   í”„ë¡œì íŠ¸ëª…: {metadata.get('project_name', 'N/A')}")
            logger.info(f"   ì„¤ëª…: {metadata.get('description', 'N/A')[:100]}...")
        else:
            logger.info("   âš ï¸ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ”§ MCP í”„ë¡¬í”„íŠ¸ í–¥ìƒ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    await test_mcp_system()
    
    print("\n" + "=" * 60)
    print("ğŸ“š ì‚¬ìš© ë°©ë²•:")
    print("1. ë‹¤ë¥¸ í”„ë¡œì íŠ¸ë¥¼ ì¸ë±ì‹±: index_project_files('/path/to/project', 'project_id')")
    print("2. í”„ë¡¬í”„íŠ¸ ê°œì„ : enhance_prompt('your prompt', 'project_id')")
    print("3. ëŒ€í™” ì €ì¥: store_conversation('user_prompt', 'ai_response', 'project_id')")
    print("4. íŒŒì¼ ê²€ìƒ‰: search_project_files('query', 'project_id')")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main()) 