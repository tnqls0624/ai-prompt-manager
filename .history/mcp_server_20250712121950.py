#!/usr/bin/env python3
"""
FastMCP ê¸°ë°˜ MCP ì„œë²„
Cursor â†’ Python FastMCP Server (SSE) â†’ LangChain/ChromaDB
"""

from mcp.server.fastmcp import FastMCP
import logging
import sys
import os
from typing import Dict, List, Any, Optional, AsyncIterator, Generator
import asyncio
import json
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from sse_starlette import EventSourceResponse
import time
from pathlib import Path
import aiofiles
import hashlib
from datetime import datetime
import aiohttp
import math

# ê¸°ì¡´ ì„œë¹„ìŠ¤ë“¤ import
from services.vector_service import VectorService
from services.prompt_enhancement_service import PromptEnhancementService
from services.file_indexing_service import FileIndexingService
from services.fast_indexing_service import FastIndexingService
from services.advanced_analytics import AdvancedAnalyticsService
from services.feedback_service import FeedbackService
from services.langchain_rag_service import LangChainRAGService
from services.file_watcher_service import FileWatcherService
from services.auto_indexing_service import AutoIndexingService
from services.error_handler import (
    error_handler,
    handle_errors,
    measure_performance,
    validate_input,
    validate_project_id,
    validate_prompt_content,
    ErrorCategory,
    ErrorLevel
)
from config import settings
from models.prompt_models import PromptHistory, PromptType
import uuid
from logging.handlers import RotatingFileHandler

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± (í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ê¸°ë³¸ê°’: /data/logs)
log_dir = os.getenv('LOG_DIR', settings.log_dir)
if log_dir == "/app/logs":  # config.py ê¸°ë³¸ê°’ì¸ ê²½ìš° data/logsë¡œ ë³€ê²½
    log_dir = "/data/logs"
os.makedirs(log_dir, exist_ok=True)

# ë¡œê¹… í•¸ë“¤ëŸ¬ ì„¤ì •
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ì½˜ì†” í•¸ë“¤ëŸ¬
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)

# íŒŒì¼ í•¸ë“¤ëŸ¬ (ë¡œí…Œì´ì…˜) - ê¸°ë³¸ê°’ ì‚¬ìš©
file_handler = RotatingFileHandler(
    filename=os.path.join(log_dir, "mcp_server.log"),
    maxBytes=10 * 1024 * 1024,  # 10MB ê¸°ë³¸ê°’
    backupCount=5,  # ë°±ì—… íŒŒì¼ 5ê°œ ê¸°ë³¸ê°’
    encoding='utf-8'
)
file_handler.setFormatter(log_formatter)

# ì—ëŸ¬ ì „ìš© íŒŒì¼ í•¸ë“¤ëŸ¬
error_file_handler = RotatingFileHandler(
    filename=os.path.join(log_dir, "mcp_server_error.log"),
    maxBytes=10 * 1024 * 1024,  # 10MB ê¸°ë³¸ê°’
    backupCount=5,  # ë°±ì—… íŒŒì¼ 5ê°œ ê¸°ë³¸ê°’
    encoding='utf-8'
)
error_file_handler.setFormatter(log_formatter)
error_file_handler.setLevel(logging.ERROR)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=getattr(logging, settings.log_level),
    handlers=[console_handler, file_handler, error_file_handler]
)

logger = logging.getLogger("mcp-server")

# FastMCP ì„œë²„ ìƒì„±
mcp = FastMCP(
    name="Prompt Enhancement MCP Server",
    debug=True,
    host="0.0.0.0",  # Docker ì»¨í…Œì´ë„ˆ ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©
    port=8000
)

# ğŸ¯ FastMCPì˜ SSE ì•± í™œìš© (í†µí•©ëœ ì ‘ê·¼ë²•)
# FastMCPëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Starletteë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ custom_routeë¡œ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
vector_service: Optional[VectorService] = None
enhancement_service: Optional[PromptEnhancementService] = None
file_indexing_service: Optional[FileIndexingService] = None
fast_indexing_service: Optional[FastIndexingService] = None
analytics_service: Optional[AdvancedAnalyticsService] = None
feedback_service: Optional[FeedbackService] = None
langchain_rag_service: Optional[LangChainRAGService] = None
file_watcher_service: Optional[FileWatcherService] = None
auto_indexing_service: Optional[AutoIndexingService] = None

# ì´ˆê¸°í™” ìƒíƒœ ê´€ë¦¬
_initialization_complete = asyncio.Event()
_services_initialized = False

# SSE ì—°ê²° ê´€ë¦¬
active_connections: Dict[str, asyncio.Queue] = {}

class SSEEventType:
    """SSE ì´ë²¤íŠ¸ íƒ€ì…"""
    ENHANCEMENT_START = "enhancement_start"
    ENHANCEMENT_PROGRESS = "enhancement_progress"
    ENHANCEMENT_COMPLETE = "enhancement_complete"
    CONTEXT_SEARCH = "context_search"
    ERROR = "error"
    HEARTBEAT = "heartbeat"

@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.CRITICAL,
    user_message="ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="service_initialization", threshold=10.0)
async def initialize_services():
    """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    global vector_service, enhancement_service, file_indexing_service, fast_indexing_service, analytics_service, feedback_service, langchain_rag_service, file_watcher_service, auto_indexing_service, _services_initialized
    
    if _services_initialized:
        logger.info("ì„œë¹„ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
        return
    
    logger.info("ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
    
    # ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vector_service = VectorService()
    analytics_service = AdvancedAnalyticsService()
    enhancement_service = PromptEnhancementService(vector_service)
    file_indexing_service = FileIndexingService(vector_service)
    fast_indexing_service = FastIndexingService(vector_service)
    feedback_service = FeedbackService(vector_service)
    langchain_rag_service = LangChainRAGService(vector_service)
    file_watcher_service = FileWatcherService(vector_service)
    auto_indexing_service = AutoIndexingService(vector_service, file_indexing_service)
    
    # ìë™ ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
    logger.info("ìë™ ë°±ê·¸ë¼ìš´ë“œ ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‹œì‘...")
    try:
        await auto_indexing_service.start()
        logger.info("ìë™ ì¸ë±ì‹± ì„œë¹„ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ìë™ ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ì´ˆê¸°í™” ì™„ë£Œ í‘œì‹œ
    _services_initialized = True
    _initialization_complete.set()
    
    logger.info("ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.HIGH,
    user_message="ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
async def ensure_services_initialized():
    """ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    if not _services_initialized:
        logger.info("ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘...")
        await initialize_services()
    
    # ì´ˆê¸°í™” ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    try:
        await asyncio.wait_for(_initialization_complete.wait(), timeout=30.0)
    except asyncio.TimeoutError:
        logger.error("ì„œë¹„ìŠ¤ ì´ˆê¸°í™” íƒ€ì„ì•„ì›ƒ")
        raise RuntimeError("ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ê°€ 30ì´ˆ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

async def store_prompt(
    content: str,
    project_id: str,
    prompt_type: str
) -> Dict[str, Any]:
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        content: ì €ì¥í•  í”„ë¡¬í”„íŠ¸ ë‚´ìš©
        project_id: í”„ë¡œì íŠ¸ ID
        prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì… ("user_query", "ai_response", "system_prompt", "enhanced_prompt")
    
    Returns:
        ì €ì¥ ê²°ê³¼ ì •ë³´
    """
    try:
        await ensure_services_initialized()
        
        # PromptHistory ê°ì²´ ìƒì„±
        prompt_history = PromptHistory(
            id=str(uuid.uuid4()),
            project_id=project_id,
            content=content,
            prompt_type=PromptType(prompt_type),
            metadata={}
        )
        
        # ë²¡í„° ì„œë¹„ìŠ¤ì— ì €ì¥
        success = await vector_service.store_prompt_history(prompt_history)
        
        if success:
            return {
                "success": True,
                "id": prompt_history.id,
                "message": "í”„ë¡¬í”„íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"
            }
        else:
            return {
                "success": False,
                "error": "ë²¡í„° ì„œë¹„ìŠ¤ ì €ì¥ ì‹¤íŒ¨"
            }
            
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e)
        }

async def create_sse_event(event_type: str, data: Dict[str, Any]) -> str:
    """SSE ì´ë²¤íŠ¸ ìƒì„±"""
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"

async def broadcast_to_connection(connection_id: str, event_type: str, data: Dict[str, Any]):
    """íŠ¹ì • ì—°ê²°ì— ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    if connection_id in active_connections:
        event_data = {
            "type": event_type,
            "data": data,
            "timestamp": time.time()
        }
        await active_connections[connection_id].put(event_data)

# SSE ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/sse/{connection_id}", methods=["GET"])
@handle_errors(
    category=ErrorCategory.NETWORK,
    level=ErrorLevel.MEDIUM,
    user_message="SSE ì—°ê²° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
async def sse_endpoint(request):
    """SSE ì—°ê²° ì—”ë“œí¬ì¸íŠ¸"""
    connection_id = request.path_params.get("connection_id")
    logger.info(f"SSE ì—°ê²° ì‹œì‘: {connection_id}")
    
    # ì—°ê²° í ìƒì„±
    event_queue = asyncio.Queue()
    active_connections[connection_id] = event_queue
    
    async def event_generator():
        try:
            # ì—°ê²° í™•ì¸ ë©”ì‹œì§€
            yield await create_sse_event("connected", {
                "connection_id": connection_id,
                "server_status": "ready"
            })
            
            while True:
                try:
                    # ì´ë²¤íŠ¸ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ 30ì´ˆ)
                    event = await asyncio.wait_for(event_queue.get(), timeout=30.0)
                    yield await create_sse_event(event["type"], event["data"])
                except asyncio.TimeoutError:
                    # í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
                    yield await create_sse_event("heartbeat", {
                        "timestamp": time.time()
                    })
                    
        except Exception as e:
            logger.error(f"SSE ì´ë²¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            yield await create_sse_event("error", {
                "error": str(e)
            })
        finally:
            # ì—°ê²° ì •ë¦¬
            if connection_id in active_connections:
                del active_connections[connection_id]
            logger.info(f"SSE ì—°ê²° ì¢…ë£Œ: {connection_id}")
    
    return EventSourceResponse(event_generator())

# ìŠ¤íŠ¸ë¦¬ë° í”„ë¡¬í”„íŠ¸ ê°œì„  ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/enhance-prompt-stream/{connection_id}", methods=["POST"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="enhance_prompt_stream", threshold=5.0)
async def enhance_prompt_stream(request):
    """ìŠ¤íŠ¸ë¦¬ë° í”„ë¡¬í”„íŠ¸ ê°œì„ """
    await ensure_services_initialized()
    
    # Path parameter ì¶”ì¶œ
    connection_id = request.path_params.get("connection_id")
    
    # ìš”ì²­ ë°ì´í„° íŒŒì‹±
    request_data = await request.json()
    prompt = request_data.get("prompt", "")
    project_id = request_data.get("project_id", "default")
    context_limit = request_data.get("context_limit", 5)
    
    # ì…ë ¥ ê²€ì¦
    if not validate_prompt_content(prompt):
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤.")
    
    if not validate_project_id(project_id):
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤.")
    
    # ê°œì„  ì‹œì‘ ì•Œë¦¼
    await broadcast_to_connection(connection_id, SSEEventType.ENHANCEMENT_START, {
        "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
        "project_id": project_id
    })
    
    # PromptEnhanceRequest ê°ì²´ ìƒì„±
    from models.prompt_models import PromptEnhanceRequest
    enhance_request = PromptEnhanceRequest(
        original_prompt=prompt,
        project_id=project_id,
        context_limit=context_limit
    )
    
    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì•Œë¦¼
    await broadcast_to_connection(connection_id, SSEEventType.CONTEXT_SEARCH, {
        "status": "searching",
        "message": "ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."
    })
    
    # í”„ë¡¬í”„íŠ¸ ê°œì„  ìˆ˜í–‰
    result = await enhancement_service.enhance_prompt(enhance_request)
    
    # ê°œì„  ì™„ë£Œ ì•Œë¦¼
    await broadcast_to_connection(connection_id, SSEEventType.ENHANCEMENT_COMPLETE, {
        "enhanced_prompt": result.enhanced_prompt,
        "confidence_score": result.confidence_score,
        "context_used": result.context_used,
        "suggestions": result.suggestions
    })
    
    return {
        "success": True,
        "message": "í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "connection_id": connection_id
    }

# ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì íŠ¸ ì¸ë±ì‹± ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/index-project-stream/{connection_id}", methods=["POST"])
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡œì íŠ¸ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="index_project_stream", threshold=30.0)
async def index_project_stream(request):
    """ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì íŠ¸ ì¸ë±ì‹±"""
    await ensure_services_initialized()
    
    # Path parameter ì¶”ì¶œ
    connection_id = request.path_params.get("connection_id")
    
    # ìš”ì²­ ë°ì´í„° íŒŒì‹±
    request_data = await request.json()
    project_path = request_data.get("project_path", "")
    project_id = request_data.get("project_id", "default")
    
    # ì…ë ¥ ê²€ì¦
    if not validate_project_id(project_id):
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤.")
    
    if not os.path.exists(project_path):
        raise HTTPException(status_code=400, detail="í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ì¸ë±ì‹± ì‹œì‘ ì•Œë¦¼
    await broadcast_to_connection(connection_id, "indexing_start", {
        "project_path": project_path,
        "project_id": project_id
    })
    
    # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜
    async def progress_callback(current: int, total: int, message: str):
        await broadcast_to_connection(connection_id, "indexing_progress", {
            "current": current,
            "total": total,
            "message": message,
            "progress": (current / total) * 100 if total > 0 else 0
        })
    
    # í†µí•© ì¸ë±ì‹± ìˆ˜í–‰
    result = await comprehensive_project_indexing(
        project_path=project_path,
        project_id=project_id,
        progress_callback=progress_callback
    )
    
    # ì¸ë±ì‹± ì™„ë£Œ ì•Œë¦¼
    await broadcast_to_connection(connection_id, "indexing_complete", {
        "result": result
    })
    
    return {
        "success": True,
        "message": "í”„ë¡œì íŠ¸ ì¸ë±ì‹±ì´ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
        "connection_id": connection_id
    }

# ğŸš€ ìƒˆë¡œìš´ íŒŒì¼ ì—…ë¡œë“œ ë° ë³‘ë ¬ ì¸ë±ì‹± API
@mcp.custom_route(path="/api/v1/upload-files", methods=["POST"])
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.MEDIUM,
    user_message="íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="upload_files", threshold=30.0)
async def upload_files(request):
    """íŒŒì¼ ì—…ë¡œë“œ ë° ë³‘ë ¬ ì¸ë±ì‹±"""
    await ensure_services_initialized()
    
    try:
        # í¼ ë°ì´í„° íŒŒì‹±
        form_data = await request.form()
        project_id = form_data.get("project_id", "default")
        project_name = form_data.get("project_name", "uploaded-project")
        
        # ì…ë ¥ ê²€ì¦
        if not validate_project_id(project_id):
            return JSONResponse(
                status_code=400,
                content={"error": "ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤.", "success": False}
            )
        
        # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬
        uploaded_files = []
        file_count = 0
        
        for key, file in form_data.items():
            if key.startswith("file_") and hasattr(file, 'filename'):
                if file.filename and file.filename.strip():
                    file_content = await file.read()
                    if len(file_content) > 0:
                        uploaded_files.append({
                            "filename": file.filename,
                            "content": file_content.decode('utf-8', errors='ignore'),
                            "size": len(file_content)
                        })
                        file_count += 1
        
        if not uploaded_files:
            return JSONResponse(
                status_code=400,
                content={"error": "ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "success": False}
            )
        
        logger.info(f"ğŸ“¤ {file_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ, ë³‘ë ¬ ì¸ë±ì‹± ì‹œì‘...")
        
        # ë³‘ë ¬ ì¸ë±ì‹± ì²˜ë¦¬
        indexed_files = []
        failed_files = []
        
        # Semaphoreë¡œ ë™ì‹œì„± ì œì–´ (ìµœëŒ€ 10ê°œ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬)
        semaphore = asyncio.Semaphore(10)
        
        async def process_uploaded_file(file_data):
            async with semaphore:
                try:
                    # íŒŒì¼ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥
                    await _store_uploaded_file_to_vector_db(
                        file_data["filename"],
                        file_data["content"],
                        project_id
                    )
                    return file_data["filename"]
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨ {file_data['filename']}: {e}")
                    return None
        
        # ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        results = await asyncio.gather(
            *[process_uploaded_file(file_data) for file_data in uploaded_files],
            return_exceptions=True
        )
        
        # ê²°ê³¼ ë¶„ë¥˜
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_files.append(uploaded_files[i]["filename"])
            elif result is not None:
                indexed_files.append(result)
            else:
                failed_files.append(uploaded_files[i]["filename"])
        
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë„ ì €ì¥
        from models.prompt_models import ProjectContext
        project_context = ProjectContext(
            project_id=project_id,
            project_name=project_name,
            description=f"ì—…ë¡œë“œëœ í”„ë¡œì íŠ¸ ({len(indexed_files)}ê°œ íŒŒì¼)",
            tech_stack=_detect_tech_stack_from_files([f["filename"] for f in uploaded_files]),
            file_patterns=list(set([f"*{Path(f['filename']).suffix}" for f in uploaded_files]))
        )
        await vector_service.store_project_context(project_context)
        
        return JSONResponse({
            "success": True,
            "project_id": project_id,
            "project_name": project_name,
            "total_files_uploaded": len(uploaded_files),
            "indexed_files_count": len(indexed_files),
            "failed_files_count": len(failed_files),
            "indexed_files": indexed_files,
            "failed_files": failed_files[:10] if failed_files else [],
            "tech_stack": project_context.tech_stack
        })
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", "success": False}
        )

# ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ API (JSON ë°©ì‹)
@mcp.custom_route(path="/api/v1/upload-batch", methods=["POST"])
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.MEDIUM,
    user_message="ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="upload_batch", threshold=60.0)
async def upload_batch(request):
    """ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ ë° ì¸ë±ì‹± (JSON ë°©ì‹)"""
    await ensure_services_initialized()
    
    try:
        # JSON ë°ì´í„° íŒŒì‹±
        request_data = await request.json()
        project_id = request_data.get("project_id", "default")
        project_name = request_data.get("project_name", "batch-project")
        files_data = request_data.get("files", [])
        
        # ì…ë ¥ ê²€ì¦
        if not validate_project_id(project_id):
            return JSONResponse(
                status_code=400,
                content={"error": "ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤.", "success": False}
            )
        
        if not files_data:
            return JSONResponse(
                status_code=400,
                content={"error": "ì—…ë¡œë“œí•  íŒŒì¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "success": False}
            )
        
        logger.info(f"ğŸ“¤ {len(files_data)}ê°œ íŒŒì¼ ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘...")
        
        # ë³‘ë ¬ ì¸ë±ì‹± ì²˜ë¦¬ (ë” í° ë°°ì¹˜ í¬ê¸°)
        indexed_files = []
        failed_files = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (100ê°œì”©)
        batch_size = 100
        semaphore = asyncio.Semaphore(50)  # í›¨ì”¬ ë†’ì€ ë™ì‹œì„±
        
        async def process_file_batch(file_data):
            async with semaphore:
                try:
                    filename = file_data.get("path", file_data.get("filename", "unknown"))
                    content = file_data.get("content", "")
                    
                    if len(content.strip()) < 10:  # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
                        return None
                    
                    await _store_uploaded_file_to_vector_db(filename, content, project_id)
                    return filename
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨ {filename}: {e}")
                    return None
        
        # ë°°ì¹˜ë³„ë¡œ ë³‘ë ¬ ì²˜ë¦¬
        for i in range(0, len(files_data), batch_size):
            batch = files_data[i:i + batch_size]
            logger.info(f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1}/{(len(files_data) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
            
            batch_results = await asyncio.gather(
                *[process_file_batch(file_data) for file_data in batch],
                return_exceptions=True
            )
            
            # ê²°ê³¼ ë¶„ë¥˜
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    failed_files.append(batch[j].get("path", "unknown"))
                elif result is not None:
                    indexed_files.append(result)
                else:
                    failed_files.append(batch[j].get("path", "unknown"))
        
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        from models.prompt_models import ProjectContext
        project_context = ProjectContext(
            project_id=project_id,
            project_name=project_name,
            description=f"ë°°ì¹˜ ì—…ë¡œë“œëœ í”„ë¡œì íŠ¸ ({len(indexed_files)}ê°œ íŒŒì¼)",
            tech_stack=_detect_tech_stack_from_files([f.get("path", "") for f in files_data]),
            file_patterns=list(set([f"*{Path(f.get('path', '')).suffix}" for f in files_data if f.get('path')]))
        )
        await vector_service.store_project_context(project_context)
        
        return JSONResponse({
            "success": True,
            "project_id": project_id,
            "project_name": project_name,
            "total_files_received": len(files_data),
            "indexed_files_count": len(indexed_files),
            "failed_files_count": len(failed_files),
            "success_rate": round((len(indexed_files) / len(files_data)) * 100, 1),
            "tech_stack": project_context.tech_stack,
            "file_patterns": project_context.file_patterns
        })
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"ë°°ì¹˜ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}", "success": False}
        )

# í—¬í¼ í•¨ìˆ˜ë“¤
async def _store_uploaded_file_to_vector_db(filename: str, content: str, project_id: str):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥"""
    try:
        # íŒŒì¼ ê²½ë¡œ ì •ë³´ ìƒì„±
        file_path = Path(filename)
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": filename,
            "file_name": file_path.name,
            "file_extension": file_path.suffix,
            "chunk_index": 0,
            "file_type": "code" if file_path.suffix in ['.py', '.js', '.ts', '.java', '.cpp', '.jsx', '.tsx'] else "documentation",
            "is_file_content": True,
            "upload_method": "network"
        }
        
        # íŒŒì¼ì´ í¬ë©´ ì²­í‚¹
        if len(content) > 16000:  # 16KB ì´ìƒì´ë©´ ì²­í‚¹
            chunks = _chunk_text_content(content)
            for i, chunk in enumerate(chunks):
                chunk_metadata = metadata.copy()
                chunk_metadata["chunk_index"] = i
                
                # ê³ ìœ  ID ìƒì„±
                chunk_id = f"{project_id}_upload_{hashlib.md5(f'{filename}_{i}'.encode()).hexdigest()}"
                
                # PromptHistory ê°ì²´ ìƒì„±
                from models.prompt_models import PromptHistory, PromptType
                prompt_history = PromptHistory(
                    id=chunk_id,
                    project_id=project_id,
                    content=chunk,
                    prompt_type=PromptType.SYSTEM_PROMPT,
                    metadata=chunk_metadata,
                    created_at=datetime.now()
                )
                
                await vector_service.store_prompt_history(prompt_history)
        else:
            # ì‘ì€ íŒŒì¼ì€ í†µì§¸ë¡œ ì €ì¥
            file_id = f"{project_id}_upload_{hashlib.md5(filename.encode()).hexdigest()}"
            
            from models.prompt_models import PromptHistory, PromptType
            prompt_history = PromptHistory(
                id=file_id,
                project_id=project_id,
                content=content,
                prompt_type=PromptType.SYSTEM_PROMPT,
                metadata=metadata,
                created_at=datetime.now()
            )
            
            await vector_service.store_prompt_history(prompt_history)
            
    except Exception as e:
        logger.error(f"íŒŒì¼ ë²¡í„° ì €ì¥ ì‹¤íŒ¨ {filename}: {e}")
        raise

def _chunk_text_content(content: str) -> List[str]:
    """í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì²­í‚¹"""
    # ê¸°ë³¸ ì²­í‚¹ (ì¤„ ë‹¨ìœ„)
    lines = content.split('\n')
    chunks = []
    current_chunk = []
    current_size = 0
    
    for line in lines:
        current_chunk.append(line)
        current_size += len(line)
        
        if current_size > 8000:  # 8KBì”© ì²­í‚¹
            chunks.append('\n'.join(current_chunk))
            current_chunk = []
            current_size = 0
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks

def _detect_tech_stack_from_files(filenames: List[str]) -> List[str]:
    """íŒŒì¼ëª…ë“¤ë¡œë¶€í„° ê¸°ìˆ  ìŠ¤íƒ ê°ì§€"""
    tech_stack = set()
    
    for filename in filenames:
        ext = Path(filename).suffix.lower()
        
        if ext in ['.py']:
            tech_stack.add('Python')
        elif ext in ['.js', '.jsx']:
            tech_stack.add('JavaScript')
        elif ext in ['.ts', '.tsx']:
            tech_stack.add('TypeScript')
        elif ext in ['.java']:
            tech_stack.add('Java')
        elif ext in ['.cpp', '.c']:
            tech_stack.add('C/C++')
        elif ext in ['.cs']:
            tech_stack.add('C#')
        elif ext in ['.go']:
            tech_stack.add('Go')
        elif ext in ['.rs']:
            tech_stack.add('Rust')
        elif ext in ['.php']:
            tech_stack.add('PHP')
        elif ext in ['.rb']:
            tech_stack.add('Ruby')
        elif ext in ['.swift']:
            tech_stack.add('Swift')
        elif ext in ['.kt']:
            tech_stack.add('Kotlin')
        elif ext in ['.vue']:
            tech_stack.add('Vue.js')
        elif ext in ['.svelte']:
            tech_stack.add('Svelte')
    
    return sorted(list(tech_stack))

# ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/health", methods=["GET"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"status": "error", "message": "ê±´ê°• ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"}
)
async def health_check(request):
    """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸"""
    health_status = error_handler.get_health_status()
    
    # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    services_status = {
        "vector_service": vector_service is not None,
        "enhancement_service": enhancement_service is not None,
        "file_indexing_service": file_indexing_service is not None,
        "fast_indexing_service": fast_indexing_service is not None,
        "analytics_service": analytics_service is not None
    }
    
    return JSONResponse({
        "status": "healthy",
        "timestamp": time.time(),
        "services": services_status,
        "health_stats": health_status,
        "active_connections": len(active_connections)
    })

# ì—ëŸ¬ í†µê³„ ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/error-stats", methods=["GET"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"error": "ì—ëŸ¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨"}
)
async def get_error_stats(request):
    """ì—ëŸ¬ í†µê³„ ì¡°íšŒ"""
    return JSONResponse({
        "error_stats": error_handler.get_error_stats(),
        "performance_stats": error_handler.get_performance_stats()
    })

# MCP ë„êµ¬ë“¤ ì •ì˜

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="enhance_prompt", threshold=3.0)
async def enhance_prompt(
    prompt: str,
    project_id: str = "default",
    context_limit: int = 5
) -> Dict[str, Any]:
    """
    AI í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ì œì•ˆì„ ì œê³µí•©ë‹ˆë‹¤.
    
    Args:
        prompt: ê°œì„ í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        context_limit: ì»¨í…ìŠ¤íŠ¸ ì œí•œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    
    Returns:
        ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ì™€ ì œì•ˆì‚¬í•­
    """
    logger.info(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ìš”ì²­: {prompt[:50]}...")
    
    # ì…ë ¥ ê²€ì¦
    if not validate_prompt_content(prompt):
        return {
            "success": False,
            "error": "ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤."
        }
    
    if not validate_project_id(project_id):
        return {
            "success": False,
            "error": "ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤."
        }
    
    await ensure_services_initialized()
    
    # PromptEnhanceRequest ê°ì²´ ìƒì„±
    from models.prompt_models import PromptEnhanceRequest
    request = PromptEnhanceRequest(
        original_prompt=prompt,
        project_id=project_id,
        context_limit=context_limit
    )
    
    result = await enhancement_service.enhance_prompt(request)
    
    logger.info("í”„ë¡¬í”„íŠ¸ ê°œì„  ì™„ë£Œ")
    return {
        "success": True,
        "enhanced_prompt": result.enhanced_prompt,
        "suggestions": result.suggestions,
        "context_used": result.context_used,
        "improvement_score": result.confidence_score
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.MEDIUM,
    user_message="ëŒ€í™” ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="store_conversation", threshold=5.0)
async def store_conversation(
    user_prompt: str,
    ai_response: str,
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    ì‚¬ìš©ìì™€ AIì˜ ëŒ€í™”ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        ai_response: AI ì‘ë‹µ
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
    
    Returns:
        ì €ì¥ ê²°ê³¼ ì •ë³´
    """
    logger.info(f"ëŒ€í™” ì €ì¥ ìš”ì²­: ì‚¬ìš©ì={user_prompt[:30]}..., AI={ai_response[:30]}...")
    
    await ensure_services_initialized()
    
    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì €ì¥
    user_result = await store_prompt(user_prompt, project_id, "user_query")
    
    # AI ì‘ë‹µ ì €ì¥  
    ai_result = await store_prompt(ai_response, project_id, "ai_response")
    
    success = user_result.get("success", False) and ai_result.get("success", False)
    
    logger.info("ëŒ€í™” ì €ì¥ ì™„ë£Œ" if success else "ëŒ€í™” ì €ì¥ ì‹¤íŒ¨")
    
    return {
        "success": success,
        "message": "ëŒ€í™”ê°€ í•™ìŠµ ë°ì´í„°ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤" if success else "ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
        "user_prompt_id": user_result.get("id"),
        "ai_response_id": ai_result.get("id")
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="search_similar_conversations", threshold=3.0)
async def search_similar_conversations(
    query: str,
    project_id: str = "default",
    limit: int = 5
) -> Dict[str, Any]:
    """
    ìœ ì‚¬í•œ ëŒ€í™”ë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: 5)
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
    """
    # vector_service.search_similar_promptsì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
    await ensure_services_initialized()
    
    try:
        similar_prompts = await vector_service.search_similar_prompts(query, project_id, limit)
        
        return {
            "success": True,
            "query": query,
            "project_id": project_id,
            "results": similar_prompts,
            "total_results": len(similar_prompts)
        }
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "error": str(e),
            "query": query,
            "project_id": project_id,
            "results": []
        }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="ëŒ€í™” íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="analyze_conversation_patterns", threshold=5.0)
async def analyze_conversation_patterns(
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    ëŒ€í™” íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
    
    Returns:
        íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"ëŒ€í™” íŒ¨í„´ ë¶„ì„ ìš”ì²­: {project_id}")
    
    # í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ë¶„ì„ë§Œ ì œê³µ
    # í–¥í›„ ì‹¤ì œ íŒ¨í„´ ë¶„ì„ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥
    
    return {
        "success": True,
        "message": "íŒ¨í„´ ë¶„ì„ ê¸°ëŠ¥ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤",
        "suggestion": "ë” ë§ì€ ëŒ€í™” ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤",
        "project_id": project_id,
        "analysis_date": "2024-12-19"
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.HIGH,
    user_message="í†µí•© í”„ë¡œì íŠ¸ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="comprehensive_project_indexing", threshold=60.0)
async def comprehensive_project_indexing(
    project_path: str,
    project_id: str = "default",
    include_file_upload: bool = True,
    include_vector_storage: bool = True,
    include_context_analysis: bool = True
) -> Dict[str, Any]:
    """
    ğŸš€ í†µí•© í”„ë¡œì íŠ¸ ì¸ë±ì‹± ë„êµ¬ - íŒŒì¼ ì „ì†¡ + ì¸ë±ì‹± + ë²¡í„°DB ì €ì¥ + ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
    
    ì´ ë„êµ¬ëŠ” í”„ë¡œì íŠ¸ì˜ ëª¨ë“  íŒŒì¼ì„ ìŠ¤ìº”í•˜ê³ , ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ”
    ì „ì²´ ê³¼ì •ì„ í•˜ë‚˜ì˜ í†µí•©ëœ ì›Œí¬í”Œë¡œìš°ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        project_path: ì¸ë±ì‹±í•  í”„ë¡œì íŠ¸ ê²½ë¡œ (ì˜ˆ: /app/lovechedule-app)
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        include_file_upload: íŒŒì¼ ì—…ë¡œë“œ/ì „ì†¡ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        include_vector_storage: ë²¡í„° DB ì €ì¥ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        include_context_analysis: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        í†µí•© ì¸ë±ì‹± ê²°ê³¼ ì •ë³´ (ëª¨ë“  ë‹¨ê³„ì˜ ìƒì„¸ ê²°ê³¼ í¬í•¨)
    """
    logger.info(f"ğŸš€ í†µí•© í”„ë¡œì íŠ¸ ì¸ë±ì‹± ì‹œì‘: {project_path}")
    
    await ensure_services_initialized()
    
    # Docker í™˜ê²½ì—ì„œ í˜¸ìŠ¤íŠ¸ íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼ ë¶ˆê°€ ì²´í¬
    if not os.path.exists(project_path):
        logger.error(f"âŒ í”„ë¡œì íŠ¸ ê²½ë¡œ ì ‘ê·¼ ë¶ˆê°€: {project_path}")
        return {
            "success": False,
            "project_id": project_id,
            "project_path": project_path,
            "error": "Docker í™˜ê²½ì—ì„œëŠ” í˜¸ìŠ¤íŠ¸ íŒŒì¼ ì‹œìŠ¤í…œì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "recommendation": {
                "message": "ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ íŒŒì¼ ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”",
                "upload_endpoints": [
                    "POST /api/v1/upload-files - ê°œë³„ íŒŒì¼ ì—…ë¡œë“œ",
                    "POST /api/v1/upload-batch - ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ"
                ],
                "client_script": "python upload_project.py --project-path {project_path} --project-id {project_id}".format(
                    project_path=project_path, project_id=project_id
                ),
                "cursor_client": "python cursor_rag_client.py upload --project-path {project_path} --project-id {project_id}".format(
                    project_path=project_path, project_id=project_id
                )
            },
            "workflow_steps": {},
            "summary": {},
            "errors": [f"ê²½ë¡œ ì ‘ê·¼ ë¶ˆê°€: {project_path}"]
        }
    
    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì‹œê°„
    total_start_time = time.time()
    
    # ê²°ê³¼ ìˆ˜ì§‘ìš© ë”•ì…”ë„ˆë¦¬
    comprehensive_result = {
        "success": True,
        "project_id": project_id,
        "project_path": project_path,
        "workflow_steps": {},
        "summary": {},
        "errors": []
    }
    
    # === 1ë‹¨ê³„: í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ===
    logger.info("ğŸ“Š 1ë‹¨ê³„: í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
    step1_start = time.time()
    
    try:
        if include_context_analysis:
            analysis_result = await fast_indexing_service._analyze_project_structure_fast(Path(project_path))
            comprehensive_result["workflow_steps"]["structure_analysis"] = {
                "success": True,
                "tech_stack": analysis_result.get('tech_stack', []),
                "file_patterns": analysis_result.get('file_patterns', []),
                "description": analysis_result.get('description', ''),
                "duration_seconds": round(time.time() - step1_start, 2)
            }
            logger.info(f"âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ: {len(analysis_result.get('tech_stack', []))}ê°œ ê¸°ìˆ  ìŠ¤íƒ")
        else:
            comprehensive_result["workflow_steps"]["structure_analysis"] = {
                "success": True,
                "skipped": True,
                "reason": "ì‚¬ìš©ìê°€ ë¹„í™œì„±í™”í•¨"
            }
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
        comprehensive_result["workflow_steps"]["structure_analysis"] = {
            "success": False,
            "error": str(e),
            "duration_seconds": round(time.time() - step1_start, 2)
        }
        comprehensive_result["errors"].append(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # === 2ë‹¨ê³„: íŒŒì¼ ìŠ¤ìº” ë° í•„í„°ë§ ===
    logger.info("ğŸ“‚ 2ë‹¨ê³„: íŒŒì¼ ìŠ¤ìº” ë° í•„í„°ë§ ì¤‘...")
    step2_start = time.time()
    
    try:
        file_paths = list(fast_indexing_service._scan_files_fast(Path(project_path)))
        comprehensive_result["workflow_steps"]["file_scanning"] = {
            "success": True,
            "total_files_found": len(file_paths),
            "supported_extensions": list(fast_indexing_service.supported_extensions),
            "ignored_directories": list(fast_indexing_service.ignore_directories),
            "duration_seconds": round(time.time() - step2_start, 2)
        }
        logger.info(f"âœ… íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ: {len(file_paths)}ê°œ íŒŒì¼ ë°œê²¬")
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        comprehensive_result["workflow_steps"]["file_scanning"] = {
            "success": False,
            "error": str(e),
            "duration_seconds": round(time.time() - step2_start, 2)
        }
        comprehensive_result["errors"].append(f"íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        comprehensive_result["success"] = False
        return comprehensive_result
    
    # === 3ë‹¨ê³„: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ì €ì¥ ===
    logger.info("ğŸ—ï¸ 3ë‹¨ê³„: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ì €ì¥ ì¤‘...")
    step3_start = time.time()
    
    try:
        if include_context_analysis:
            from models.prompt_models import ProjectContext
            
            project_context = ProjectContext(
                project_id=project_id,
                project_name=Path(project_path).name,
                description=analysis_result.get('description', ''),
                tech_stack=analysis_result.get('tech_stack', []),
                file_patterns=analysis_result.get('file_patterns', [])
            )
            
            context_saved = await vector_service.store_project_context(project_context)
            comprehensive_result["workflow_steps"]["context_storage"] = {
                "success": context_saved,
                "project_name": project_context.project_name,
                "tech_stack_count": len(project_context.tech_stack),
                "file_patterns_count": len(project_context.file_patterns),
                "duration_seconds": round(time.time() - step3_start, 2)
            }
            logger.info(f"âœ… í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì €ì¥ {'ì„±ê³µ' if context_saved else 'ì‹¤íŒ¨'}")
        else:
            comprehensive_result["workflow_steps"]["context_storage"] = {
                "success": True,
                "skipped": True,
                "reason": "ì‚¬ìš©ìê°€ ë¹„í™œì„±í™”í•¨"
            }
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        comprehensive_result["workflow_steps"]["context_storage"] = {
            "success": False,
            "error": str(e),
            "duration_seconds": round(time.time() - step3_start, 2)
        }
        comprehensive_result["errors"].append(f"ì»¨í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # === 4ë‹¨ê³„: ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ===
    logger.info("ğŸ§  4ë‹¨ê³„: ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    step4_start = time.time()
    
    indexed_files = []
    
    try:
        if include_vector_storage:
            # ê³ ì† ì¸ë±ì‹± ì„œë¹„ìŠ¤ ì‚¬ìš©
            result = await fast_indexing_service.index_project_files_fast(project_path, project_id)
            
            if result.get("success"):
                indexed_files = result.get("indexed_files", [])
                comprehensive_result["workflow_steps"]["vector_storage"] = {
                    "success": True,
                    "indexed_files_count": result.get("indexed_files_count", 0),
                    "failed_files_count": result.get("failed_files_count", 0),
                    "files_per_second": result.get("files_per_second", 0),
                    "duration_seconds": round(time.time() - step4_start, 2)
                }
                logger.info(f"âœ… ë²¡í„° ì €ì¥ ì™„ë£Œ: {result.get('indexed_files_count', 0)}ê°œ íŒŒì¼")
            else:
                raise Exception(result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì¸ë±ì‹± ì˜¤ë¥˜"))
        else:
            comprehensive_result["workflow_steps"]["vector_storage"] = {
                "success": True,
                "skipped": True,
                "reason": "ì‚¬ìš©ìê°€ ë¹„í™œì„±í™”í•¨"
            }
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        comprehensive_result["workflow_steps"]["vector_storage"] = {
            "success": False,
            "error": str(e),
            "duration_seconds": round(time.time() - step4_start, 2)
        }
        comprehensive_result["errors"].append(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        comprehensive_result["success"] = False
    
    # === 5ë‹¨ê³„: ìµœì¢… ê²€ì¦ ë° ìš”ì•½ ===
    logger.info("âœ… 5ë‹¨ê³„: ìµœì¢… ê²€ì¦ ë° ìš”ì•½ ìƒì„± ì¤‘...")
    
    total_duration = time.time() - total_start_time
    
    # ì „ì²´ ìš”ì•½ ìƒì„±
    comprehensive_result["summary"] = {
        "total_duration_seconds": round(total_duration, 2),
        "total_files_scanned": len(file_paths),
        "total_files_indexed": len(indexed_files) if include_vector_storage else 0,
        "success_rate": (len(indexed_files) / len(file_paths) * 100) if file_paths else 0,
        "workflow_completion": {
            "structure_analysis": comprehensive_result["workflow_steps"]["structure_analysis"]["success"],
            "file_scanning": comprehensive_result["workflow_steps"]["file_scanning"]["success"],
            "context_storage": comprehensive_result["workflow_steps"]["context_storage"]["success"],
            "vector_storage": comprehensive_result["workflow_steps"]["vector_storage"]["success"]
        },
        "performance_metrics": {
            "files_per_second": round(len(indexed_files) / total_duration, 2) if total_duration > 0 else 0,
            "scan_to_index_ratio": round((len(indexed_files) / len(file_paths)) * 100, 1) if file_paths else 0
        }
    }
    
    # ìµœì¢… ì„±ê³µ ì—¬ë¶€ ê²°ì •
    critical_steps = ["file_scanning", "vector_storage"]
    comprehensive_result["success"] = all(
        comprehensive_result["workflow_steps"][step]["success"] 
        for step in critical_steps
    )
    
    if comprehensive_result["success"]:
        logger.info(f"ğŸ‰ í†µí•© ì¸ë±ì‹± ì™„ë£Œ! {len(indexed_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ({total_duration:.2f}ì´ˆ)")
    else:
        logger.error(f"âŒ í†µí•© ì¸ë±ì‹± ë¶€ë¶„ ì‹¤íŒ¨. ì˜¤ë¥˜: {len(comprehensive_result['errors'])}ê°œ")
    
    return comprehensive_result

@mcp.tool()
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"error": "ê³ ì† ì¸ë±ì‹± í†µê³„ ì¡°íšŒ ì‹¤íŒ¨"}
)
@measure_performance(operation_name="get_fast_indexing_stats", threshold=3.0)
async def get_fast_indexing_stats() -> Dict[str, Any]:
    """
    ê³ ì† ì¸ë±ì‹± ì„œë¹„ìŠ¤ì˜ ì„±ëŠ¥ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì„±ëŠ¥ ì„¤ì • ë° í†µê³„ ì •ë³´
    """
    await ensure_services_initialized()
    
    stats = fast_indexing_service.get_performance_stats()
    
    return {
        "success": True,
        "performance_settings": stats,
        "optimization_features": [
            "ë³‘ë ¬ íŒŒì¼ ì²˜ë¦¬ (ìµœëŒ€ 20ê°œ ë™ì‹œ)",
            "ë°°ì¹˜ ì„ë² ë”© ìƒì„± (10ê°œì”©)",
            "íŒŒì¼ í•´ì‹œ ìºì‹±",
            "ë” í° ì²­í¬ í¬ê¸° (2KB)",
            "ë¹„ë™ê¸° íŒŒì¼ I/O",
            "Thread Pool ì‚¬ìš©",
            "ìŠ¤ë§ˆíŠ¸ íŒŒì¼ í•„í„°ë§"
        ],
        "speed_improvements": {
            "concurrent_processing": "5-10x faster",
            "batch_embeddings": "3-5x fewer API calls",
            "file_caching": "Skip already processed files",
            "larger_chunks": "Fewer storage operations"
        }
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡œì íŠ¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="search_project_files", threshold=3.0)
async def search_project_files(
    query: str,
    project_id: str = "default",
    file_type: str = "all",
    limit: int = 10
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ íŒŒì¼ ë‚´ìš©ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  ë‚´ìš©
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        file_type: íŒŒì¼ íƒ€ì… í•„í„° ("code", "documentation", "all")
        limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ (ê¸°ë³¸ê°’: 10)
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼
    """
    logger.info(f"í”„ë¡œì íŠ¸ íŒŒì¼ ê²€ìƒ‰ ìš”ì²­: {query} (íƒ€ì…: {file_type})")
    
    await ensure_services_initialized()
    
    # í”„ë¡œì íŠ¸ ë‚´ íŒŒì¼ ê²€ìƒ‰
    results = await vector_service.search_similar_prompts(
        query=query,
        project_id=project_id,
        limit=limit
    )
    
    # íŒŒì¼ ì»¨í…ì¸ ë§Œ í•„í„°ë§
    file_results = [
        result for result in results 
        if result.get('metadata', {}).get('is_file_content', False)
    ]
    
    # íŒŒì¼ íƒ€ì… í•„í„°ë§
    if file_type != "all":
        file_results = [
            result for result in file_results 
            if result.get('metadata', {}).get('file_type', '') == file_type
        ]
    
    formatted_results = []
    for result in file_results:
        metadata = result.get('metadata', {})
        formatted_results.append({
            "file_path": metadata.get('file_path', ''),
            "file_name": metadata.get('file_name', ''),
            "file_extension": metadata.get('file_extension', ''),
            "content_preview": result.get('content', '')[:200] + "...",
            "similarity": result.get('similarity', 0),
            "chunk_index": metadata.get('chunk_index', 0),
            "file_type": metadata.get('file_type', 'unknown')
        })
    
    return {
        "success": True,
        "query": query,
        "project_id": project_id,
        "file_type": file_type,
        "results_count": len(formatted_results),
        "results": formatted_results
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="get_project_context_info", threshold=3.0)
async def get_project_context_info(
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
    
    Returns:
        í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    """
    logger.info(f"í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ: {project_id}")
    
    await ensure_services_initialized()
    
    context = await vector_service.get_project_context(project_id)
    
    if context:
        return {
            "success": True,
            "project_id": project_id,
            "context": context
        }
    else:
        return {
            "success": False,
            "message": f"í”„ë¡œì íŠ¸ '{project_id}'ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "suggestion": "ë¨¼ì € í”„ë¡œì íŠ¸ íŒŒì¼ì„ ì¸ë±ì‹±í•´ì£¼ì„¸ìš”."
        }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"status": "error", "message": "ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨"}
)
async def get_server_status() -> Dict[str, Any]:
    """
    ì„œë²„ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì„œë²„ ìƒíƒœ ì •ë³´
    """
    await ensure_services_initialized()
    return {
        "status": "running",
        "server_name": "Prompt Enhancement MCP Server",
        "version": "1.0.0",
        "services": {
            "vector_service": vector_service is not None,
            "enhancement_service": enhancement_service is not None,
            "file_indexing_service": file_indexing_service is not None,
            "fast_indexing_service": fast_indexing_service is not None,
            "analytics_service": analytics_service is not None
        },
        "transport": "SSE",
        "capabilities": [
            "prompt_enhancement",
            "vector_search",
            "conversation_storage",
            "pattern_analysis",
            "file_indexing",
            "fast_parallel_indexing",
            "project_analysis",
            "advanced_analytics",
            "clustering_analysis",
            "keyword_extraction",
            "trend_analysis"
        ]
    }

# ê³ ê¸‰ ë¶„ì„ ë„êµ¬ë“¤

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="analyze_prompt_patterns", threshold=5.0)
async def analyze_prompt_patterns(
    project_id: str = "default",
    n_clusters: int = 5
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ì„ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
    
    Returns:
        í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ë¶„ì„ ì‹œì‘: {project_id}")
    
    await ensure_services_initialized()
    
    # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    prompts = await vector_service.search_similar_prompts(
        query="",  # ë¹ˆ ì¿¼ë¦¬ë¡œ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ê²€ìƒ‰
        project_id=project_id,
        limit=100
    )
    
    if len(prompts) < n_clusters:
        return {
            "success": False,
            "message": f"ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ {n_clusters}ê°œì˜ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(prompts)}ê°œ"
        }
    
    # ì„ë² ë”©ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    embeddings = []
    texts = []
    for prompt in prompts:
        # ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë”ë¯¸ ì„ë² ë”©
        embedding = prompt.get('embedding', [0.0] * 1536)  # OpenAI ê¸°ë³¸ ì°¨ì›
        embeddings.append(embedding)
        texts.append(prompt.get('content', ''))
    
    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    clustering_result = await analytics_service.cluster_prompts(
        prompt_embeddings=embeddings,
        prompt_texts=texts,
        n_clusters=n_clusters
    )
    
    return {
        "success": True,
        "project_id": project_id,
        "total_prompts": len(prompts),
        "clustering_result": clustering_result
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="extract_prompt_keywords", threshold=5.0)
async def extract_prompt_keywords(
    project_id: str = "default",
    max_features: int = 20
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ TF-IDFë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        max_features: ì¶”ì¶œí•  ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 20)
    
    Returns:
        í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼
    """
    logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘: {project_id}")
    
    await ensure_services_initialized()
    
    # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    prompts = await vector_service.search_similar_prompts(
        query="",
        project_id=project_id,
        limit=100
    )
    
    if not prompts:
        return {
            "success": False,
            "message": "ë¶„ì„í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [prompt.get('content', '') for prompt in prompts]
    
    # TF-IDF íŠ¹ì„± ì¶”ì¶œ
    features_result = await analytics_service.extract_text_features(texts)
    
    return {
        "success": True,
        "project_id": project_id,
        "total_prompts": len(prompts),
        "keywords": features_result.get('top_features', [])[:max_features],
        "vocabulary_size": features_result.get('vocabulary_size', 0)
    }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="analyze_prompt_trends", threshold=5.0)
async def analyze_prompt_trends(
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
    
    Returns:
        íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
    """
    logger.info(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: {project_id}")
    
    await ensure_services_initialized()
    
    # í”„ë¡œì íŠ¸ì˜ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    prompts = await vector_service.search_similar_prompts(
        query="",
        project_id=project_id,
        limit=200
    )
    
    if not prompts:
        return {
            "success": False,
            "message": "ë¶„ì„í•  í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ë°ì´í„° êµ¬ì„±
    prompt_data = []
    for prompt in prompts:
        prompt_data.append({
            "content": prompt.get('content', ''),
            "created_at": prompt.get('metadata', {}).get('created_at'),
            "prompt_type": prompt.get('metadata', {}).get('prompt_type')
        })
    
    # íŠ¸ë Œë“œ ë¶„ì„
    trends = await analytics_service.analyze_prompt_trends(prompt_data)
    
    return {
        "success": True,
        "project_id": project_id,
        "analysis_period": "ì „ì²´ ê¸°ê°„",
        "trends": trends
    }

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤

@mcp.prompt()
def create_enhanced_prompt(topic: str, context: str = "") -> str:
    """
    ì£¼ì œì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        topic: í”„ë¡¬í”„íŠ¸ ì£¼ì œ
        context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    
    Returns:
        ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    """
    logger.info(f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±: {topic}")
    
    base_prompt = f"""
ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ì£¼ì œ: {topic}
"""
    
    if context:
        base_prompt += f"\nì»¨í…ìŠ¤íŠ¸: {context}\n"
    
    base_prompt += """
ìš”êµ¬ì‚¬í•­:
1. ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­
2. ì˜ˆìƒ ê²°ê³¼ë¬¼ ëª…ì‹œ
3. í’ˆì§ˆ ê¸°ì¤€ í¬í•¨
4. ë‹¨ê³„ë³„ ì ‘ê·¼ ë°©ë²• ì œì‹œ

ìœ„ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
"""
    
    return base_prompt

# ë¦¬ì†ŒìŠ¤ë“¤

@mcp.resource("prompt-history://projects/{project_id}")
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="get_prompt_history", threshold=3.0)
async def get_prompt_history(project_id: str) -> str:
    """
    íŠ¹ì • í”„ë¡œì íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì
    
    Returns:
        í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ì •ë³´
    """
    logger.info(f"í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ìš”ì²­: {project_id}")
    
    await ensure_services_initialized()
    
    # ìµœê·¼ í”„ë¡¬í”„íŠ¸ë“¤ ê²€ìƒ‰
    results = await vector_service.search_similar_prompts(
        query=project_id,  # í”„ë¡œì íŠ¸ IDë¡œ ê²€ìƒ‰
        project_id=project_id,
        limit=10
    )
    
    history = f"í”„ë¡œì íŠ¸ '{project_id}'ì˜ í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬:\n\n"
    
    for i, result in enumerate(results, 1):
        history += f"{i}. {result.get('prompt', '')[:100]}...\n"
        history += f"   ì‹œê°„: {result.get('timestamp', 'N/A')}\n"
        history += f"   íƒ€ì…: {result.get('prompt_type', 'N/A')}\n\n"
    
    return history

@mcp.resource("server-info://status")
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"error": "ì„œë²„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨"}
)
async def get_server_info() -> str:
    """
    ì„œë²„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì„œë²„ ì •ë³´ ë¬¸ìì—´
    """
    try:
        status = await get_server_status()
        
        info = f"""
MCP ì„œë²„ ì •ë³´:
- ì´ë¦„: {status.get('server_name', 'N/A')}
- ë²„ì „: {status.get('version', 'N/A')}
- ìƒíƒœ: {status.get('status', 'N/A')}
- ì „ì†¡ ë°©ì‹: {status.get('transport', 'N/A')}

ì§€ì› ê¸°ëŠ¥:
"""
        
        for capability in status.get('capabilities', []):
            info += f"- {capability}\n"
        
        return info
        
    except Exception as e:
        logger.error(f"ì„œë²„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return f"ì„œë²„ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"

# ğŸ”„ í”¼ë“œë°± ê´€ë ¨ MCP íˆ´ë“¤
@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”¼ë“œë°± ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="submit_user_feedback", threshold=3.0)
async def submit_user_feedback(
    enhancement_id: str,
    original_prompt: str,
    enhanced_prompt: str,
    project_id: str = "default",
    feedback_type: str = "accept",
    user_rating: int = None,
    user_comment: str = None,
    execution_success: bool = False,
    code_accepted: bool = False,
    time_to_success: float = None
) -> Dict[str, Any]:
    """
    ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ
    
    Args:
        enhancement_id: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ID
        original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
        enhanced_prompt: ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
        project_id: í”„ë¡œì íŠ¸ ID
        feedback_type: í”¼ë“œë°± íƒ€ì… (accept, reject, partial_accept, modify)
        user_rating: ì‚¬ìš©ì í‰ì  (1-5)
        user_comment: ì‚¬ìš©ì ì½”ë©˜íŠ¸
        execution_success: ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€
        code_accepted: ì½”ë“œ ìˆ˜ë½ ì—¬ë¶€
        time_to_success: ì„±ê³µê¹Œì§€ ê±¸ë¦° ì‹œê°„ (ì´ˆ)
    
    Returns:
        í”¼ë“œë°± ë¶„ì„ ê²°ê³¼
    """
    await ensure_services_initialized()
    
    try:
        from models.prompt_models import UserFeedback, FeedbackType
        
        # í”¼ë“œë°± ê°ì²´ ìƒì„±
        feedback = UserFeedback(
            enhancement_id=enhancement_id,
            original_prompt=original_prompt,
            enhanced_prompt=enhanced_prompt,
            project_id=project_id,
            feedback_type=FeedbackType(feedback_type),
            user_rating=user_rating,
            user_comment=user_comment,
            execution_success=execution_success,
            code_accepted=code_accepted,
            time_to_success=time_to_success
        )
        
        # í”¼ë“œë°± ì²˜ë¦¬
        analysis = await feedback_service.submit_feedback(feedback)
        
        return {
            "status": "success",
            "message": "í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤",
            "analysis": {
                "enhancement_id": analysis.enhancement_id,
                "original_score": analysis.original_score,
                "adjusted_score": analysis.feedback_adjusted_score,
                "impact": analysis.feedback_impact,
                "recommendation": analysis.recommendation
            }
        }
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± ì œì¶œ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": f"í”¼ë“œë°± ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="get_feedback_statistics", threshold=3.0)
async def get_feedback_statistics(
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ë³„ í”¼ë“œë°± í†µê³„ ì¡°íšŒ
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ID
    
    Returns:
        í”¼ë“œë°± í†µê³„ ì •ë³´
    """
    await ensure_services_initialized()
    
    try:
        stats = await feedback_service.get_project_feedback_stats(project_id)
        return {
            "status": "success",
            "project_id": project_id,
            "statistics": stats
        }
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": f"í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”¼ë“œë°± íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="analyze_feedback_patterns", threshold=5.0)
async def analyze_feedback_patterns(
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    í”„ë¡œì íŠ¸ë³„ í”¼ë“œë°± íŒ¨í„´ ë¶„ì„
    
    Args:
        project_id: í”„ë¡œì íŠ¸ ID
    
    Returns:
        í”¼ë“œë°± íŒ¨í„´ ë¶„ì„ ê²°ê³¼
    """
    await ensure_services_initialized()
    
    try:
        patterns = await feedback_service.analyze_project_patterns(project_id)
        return {
            "status": "success",
            "project_id": project_id,
            "patterns": patterns
        }
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": f"í”¼ë“œë°± íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

@mcp.tool()
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”„ë¡¬í”„íŠ¸ ì¶”ì²œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="get_prompt_recommendations", threshold=3.0)
async def get_prompt_recommendations(
    prompt: str,
    project_id: str = "default"
) -> Dict[str, Any]:
    """
    í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì¶”ì²œì‚¬í•­ ì¡°íšŒ
    
    Args:
        prompt: ë¶„ì„í•  í”„ë¡¬í”„íŠ¸
        project_id: í”„ë¡œì íŠ¸ ID
    
    Returns:
        í”„ë¡¬í”„íŠ¸ ì¶”ì²œì‚¬í•­
    """
    await ensure_services_initialized()
    
    try:
        recommendations = await feedback_service.get_recommendations_for_prompt(prompt, project_id)
        return {
            "status": "success",
            "prompt": prompt,
            "project_id": project_id,
            "recommendations": recommendations
        }
        
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ ì¶”ì²œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {
            "status": "error",
            "message": f"í”„ë¡¬í”„íŠ¸ ì¶”ì²œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }

# ğŸ”„ í”¼ë“œë°± ê´€ë ¨ FastAPI ì—”ë“œí¬ì¸íŠ¸ë“¤
@mcp.custom_route(path="/api/v1/feedback", methods=["POST"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”¼ë“œë°± ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="submit_feedback", threshold=3.0)
async def submit_feedback_endpoint(request):
    """ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ"""
    await ensure_services_initialized()
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        
        # UserFeedback ê°ì²´ ìƒì„±
        from models.prompt_models import UserFeedback, FeedbackType
        feedback = UserFeedback(
            enhancement_id=data.get("enhancement_id"),
            original_prompt=data.get("original_prompt"),
            enhanced_prompt=data.get("enhanced_prompt"),
            project_id=data.get("project_id", "default"),
            feedback_type=FeedbackType(data.get("feedback_type")),
            user_rating=data.get("user_rating"),
            user_comment=data.get("user_comment"),
            execution_success=data.get("execution_success", False),
            code_accepted=data.get("code_accepted", False),
            time_to_success=data.get("time_to_success")
        )
        
        # í”¼ë“œë°± ì²˜ë¦¬
        analysis = await feedback_service.submit_feedback(feedback)
        
        return {
            "status": "success",
            "message": "í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤",
            "analysis": {
                "enhancement_id": analysis.enhancement_id,
                "original_score": analysis.original_score,
                "adjusted_score": analysis.feedback_adjusted_score,
                "impact": analysis.feedback_impact,
                "recommendation": analysis.recommendation
            }
        }
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± ì œì¶œ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@mcp.custom_route(path="/api/v1/feedback/stats/{project_id}", methods=["GET"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="get_feedback_stats", threshold=3.0)
async def get_feedback_stats_endpoint(request):
    """í”„ë¡œì íŠ¸ë³„ í”¼ë“œë°± í†µê³„"""
    await ensure_services_initialized()
    
    # Path parameter ì¶”ì¶œ
    project_id = request.path_params.get("project_id")
    
    try:
        stats = await feedback_service.get_project_feedback_stats(project_id)
        return {
            "status": "success",
            "project_id": project_id,
            "statistics": stats
        }
    except Exception as e:
        logger.error(f"í”¼ë“œë°± í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ¯ LangChain RAG ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ë“¤

@mcp.custom_route(path="/api/v1/rag/enhance-prompt", methods=["POST"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="rag_enhance_prompt", threshold=10.0)
async def rag_enhance_prompt(request):
    """RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„  ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        user_prompt = data.get("prompt", "")
        project_id = data.get("project_id", "default")
        context_limit = data.get("context_limit", 5)
        
        # ì…ë ¥ ê²€ì¦
        if not user_prompt.strip():
            return JSONResponse({"error": "í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤", "success": False})
        
        # RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í–¥ìƒ
        result = await langchain_rag_service.generate_enhanced_prompt(
            user_prompt=user_prompt,
            project_id=project_id,
            context_limit=context_limit
        )
        
        return JSONResponse(result)
        
    except Exception as e:
        logger.error(f"RAG ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return JSONResponse({"error": str(e), "success": False})

@mcp.custom_route(path="/api/v1/rag/generate-code", methods=["POST"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="RAG ê¸°ë°˜ ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="rag_generate_code", threshold=15.0)
async def rag_generate_code(request):
    """RAG ê¸°ë°˜ ì½”ë“œ ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        user_prompt = data.get("prompt", "")
        project_id = data.get("project_id", "default")
        context_limit = data.get("context_limit", 5)
        
        # ì…ë ¥ ê²€ì¦
        if not user_prompt.strip():
            return {"error": "í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤", "success": False}
        
        # RAG ê¸°ë°˜ ì½”ë“œ ìƒì„±
        result = await langchain_rag_service.generate_code_with_rag(
            user_prompt=user_prompt,
            project_id=project_id,
            context_limit=context_limit
        )
        
        return result
        
    except Exception as e:
        logger.error(f"RAG ê¸°ë°˜ ì½”ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "success": False}

@mcp.custom_route(path="/api/v1/rag/search-summarize", methods=["POST"])
@handle_errors(
    category=ErrorCategory.AI_SERVICE,
    level=ErrorLevel.MEDIUM,
    user_message="RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="rag_search_summarize", threshold=10.0)
async def rag_search_summarize(request):
    """RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ìš”ì•½ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        query = data.get("query", "")
        project_id = data.get("project_id", "default")
        limit = data.get("limit", 3)
        
        # ì…ë ¥ ê²€ì¦
        if not query.strip():
            return {"error": "ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤", "success": False}
        
        # RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ìš”ì•½
        result = await langchain_rag_service.search_and_summarize(
            query=query,
            project_id=project_id,
            limit=limit
        )
        
        return result
        
    except Exception as e:
        logger.error(f"RAG ê¸°ë°˜ ê²€ìƒ‰ ë° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "success": False}

# ğŸ¯ íŒŒì¼ ì™€ì²˜ ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ë“¤

@mcp.custom_route(path="/api/v1/watcher/start", methods=["POST"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.MEDIUM,
    user_message="íŒŒì¼ ê°ì‹œ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="start_file_watcher", threshold=5.0)
async def start_file_watcher(request):
    """íŒŒì¼ ê°ì‹œ ì‹œì‘ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        project_path = data.get("project_path", "")
        project_id = data.get("project_id", "default")
        recursive = data.get("recursive", True)
        auto_upload = data.get("auto_upload", True)
        
        # ì…ë ¥ ê²€ì¦
        if not project_path.strip():
            return {"error": "í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤", "success": False}
        
        # íŒŒì¼ ê°ì‹œ ì‹œì‘
        result = await file_watcher_service.start_watching_project(
            project_path=project_path,
            project_id=project_id,
            recursive=recursive,
            auto_upload=auto_upload
        )
        
        return result
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ê°ì‹œ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "success": False}

@mcp.custom_route(path="/api/v1/watcher/stop", methods=["POST"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.MEDIUM,
    user_message="íŒŒì¼ ê°ì‹œ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="stop_file_watcher", threshold=3.0)
async def stop_file_watcher(request):
    """íŒŒì¼ ê°ì‹œ ì¤‘ì§€ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        data = await request.json()
        project_id = data.get("project_id", "default")
        
        # ì…ë ¥ ê²€ì¦
        if not project_id.strip():
            return {"error": "í”„ë¡œì íŠ¸ IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤", "success": False}
        
        # íŒŒì¼ ê°ì‹œ ì¤‘ì§€
        result = await file_watcher_service.stop_watching_project(project_id)
        
        return result
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ê°ì‹œ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "success": False}

@mcp.custom_route(path="/api/v1/watcher/status", methods=["GET"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"error": "ê°ì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨", "success": False}
)
@measure_performance(operation_name="get_watcher_status", threshold=3.0)
async def get_watcher_status(request):
    """íŒŒì¼ ê°ì‹œ ìƒíƒœ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        await ensure_services_initialized()
        
        # ê°ì‹œ ìƒíƒœ ì¡°íšŒ
        result = await file_watcher_service.get_watching_status()
        
        return result
        
    except Exception as e:
        logger.error(f"ê°ì‹œ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": str(e), "success": False}

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@mcp.custom_route(path="/api/v1/heartbeat", methods=["GET"])
@handle_errors(
    category=ErrorCategory.SYSTEM,
    level=ErrorLevel.LOW,
    return_on_error={"status": "unhealthy", "message": "í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨"}
)
async def heartbeat(request):
    """Docker í—¬ìŠ¤ì²´í¬ìš© ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        status = await get_server_status()
        return JSONResponse({
            "status": "healthy",
            "message": "MCP ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤",
            "services": status.get("services", {}),
            "timestamp": asyncio.get_event_loop().time()
        })
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return JSONResponse({
            "status": "unhealthy", 
            "message": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}",
            "timestamp": asyncio.get_event_loop().time()
        })

@mcp.tool()
@handle_errors(
    category=ErrorCategory.DATABASE,
    level=ErrorLevel.HIGH,
    user_message="ë„¤íŠ¸ì›Œí¬ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
)
@measure_performance(operation_name="network_project_upload", threshold=120.0)
async def network_project_upload(
    project_path: str,
    project_id: str = "default",
    project_name: str = None,
    max_workers: int = 20,
    batch_size: int = 200
) -> Dict[str, Any]:
    """
    ğŸš€ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ê³ ì„±ëŠ¥ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ë° ì¸ë±ì‹±
    
    ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ë“¤ì„ ì½ì–´ HTTP APIë¥¼ í†µí•´ ì—…ë¡œë“œí•˜ê³  ë²¡í„° ì¸ë±ì‹±í•©ë‹ˆë‹¤.
    Docker í™˜ê²½ì—ì„œ í˜¸ìŠ¤íŠ¸ íŒŒì¼ ì‹œìŠ¤í…œì— ì ‘ê·¼í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        project_path: ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê²½ë¡œ
        project_id: í”„ë¡œì íŠ¸ ì‹ë³„ì (ê¸°ë³¸ê°’: "default")
        project_name: í”„ë¡œì íŠ¸ ì´ë¦„ (ê¸°ë³¸ê°’: ê²½ë¡œì—ì„œ ì¶”ì¶œ)
        max_workers: ë³‘ë ¬ íŒŒì¼ ì½ê¸° ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 20)
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 200)
    
    Returns:
        ì—…ë¡œë“œ ê²°ê³¼ ë° ì„±ëŠ¥ í†µê³„
    """
    import aiohttp
    import aiofiles
    import math
    from pathlib import Path
    
    logger.info(f"ğŸš€ ë„¤íŠ¸ì›Œí¬ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ì‹œì‘: {project_path}")
    
    # ì…ë ¥ ê²€ì¦
    if not validate_project_id(project_id):
        return {
            "success": False,
            "error": "ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ IDì…ë‹ˆë‹¤."
        }
    
    project_path = Path(project_path).resolve()
    
    if not project_path.exists():
        return {
            "success": False,
            "error": f"í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_path}"
        }
    
    if not project_name:
        project_name = project_path.name
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ìë“¤
    SUPPORTED_EXTENSIONS = {
        '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.cs',
        '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala',
        '.md', '.txt', '.rst', '.asciidoc',
        '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg',
        '.sql', '.sh', '.bash', '.ps1',
        '.html', '.css', '.scss', '.sass', '.less',
        '.vue', '.svelte', '.astro'
    }
    
    # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ë“¤
    IGNORE_DIRECTORIES = {
        'node_modules', 'bower_components', '__pycache__', '.pytest_cache',
        '.mypy_cache', 'venv', 'env', '.env', '.git', '.svn', '.hg',
        '.vscode', '.idea', 'dist', 'build', 'target', 'out', '.next',
        'bin', 'obj', 'Debug', 'Release', 'vendor', 'pkg', 'cache',
        'tmp', 'temp', 'coverage', 'logs', 'assets', 'public', 'static',
        'chroma_db'
    }
    
    # ë¬´ì‹œí•  íŒŒì¼ë“¤
    IGNORE_FILES = {
        '.gitignore', '.dockerignore', '.env', '.env.local',
        'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml',
        'poetry.lock', 'Pipfile.lock', 'pdm.lock',
        'composer.lock', 'Gemfile.lock', 'Cargo.lock',
        'go.sum', 'mix.lock', 'pubspec.lock'
    }
    
    async def read_file_async(file_path: Path) -> Dict[str, Any]:
        """ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°"""
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬ (10MB ì´ìƒì€ ì œì™¸)
            if file_path.stat().st_size > 10 * 1024 * 1024:  # 10MB
                return None
            
            # ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°
            async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = await f.read()
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
            if len(content.strip()) < 10:
                return None
            
            relative_path = file_path.relative_to(project_path)
            return {
                "path": str(relative_path),
                "content": content,
                "size": len(content)
            }
            
        except Exception as e:
            logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    # íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
    logger.info(f"ğŸ“‚ í”„ë¡œì íŠ¸ ìŠ¤ìº” ì¤‘: {project_path}")
    file_paths = []
    
    for root, dirs, files in os.walk(project_path):
        # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ ì œê±°
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRECTORIES]
        
        for file in files:
            if file in IGNORE_FILES:
                continue
            
            # .ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìˆ¨ê¹€ íŒŒì¼ ì œì™¸
            if file.startswith('.'):
                continue
                
            file_path = Path(root) / file
            
            # ì§€ì›í•˜ëŠ” í™•ì¥ìë§Œ ì²˜ë¦¬
            if file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                file_paths.append(file_path)
    
    logger.info(f"ğŸ“‚ {len(file_paths)}ê°œ íŒŒì¼ ë°œê²¬, ë³‘ë ¬ ì½ê¸° ì‹œì‘...")
    
    # íŒŒì¼ë“¤ì„ ë¹„ë™ê¸° ë³‘ë ¬ë¡œ ì½ê¸°
    semaphore = asyncio.Semaphore(max_workers)
    
    async def read_with_semaphore(file_path):
        async with semaphore:
            return await read_file_async(file_path)
    
    # ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì½ê¸°
    results = await asyncio.gather(
        *[read_with_semaphore(file_path) for file_path in file_paths],
        return_exceptions=True
    )
    
    # ìœ íš¨í•œ íŒŒì¼ë§Œ í•„í„°ë§
    files_data = []
    for result in results:
        if result is not None and not isinstance(result, Exception):
            files_data.append(result)
    
    logger.info(f"âœ… {len(files_data)}ê°œ íŒŒì¼ ì½ê¸° ì™„ë£Œ")
    
    if not files_data:
        return {
            "success": False,
            "error": "ì—…ë¡œë“œí•  ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
        }
    
    # íŒŒì¼ í¬ê¸°ë³„ í†µê³„
    total_size = sum(f["size"] for f in files_data)
    logger.info(f"ğŸ“Š ì´ í¬ê¸°: {total_size / 1024:.1f} KB")
    
    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    total_batches = math.ceil(len(files_data) / batch_size)
    logger.info(f"ğŸš€ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì—…ë¡œë“œ ì‹œì‘...")
    
    start_time = time.time()
    
    # ë°°ì¹˜ ì—…ë¡œë“œ í•¨ìˆ˜
    async def upload_batch(files_batch: List[Dict[str, Any]], batch_num: int):
        upload_data = {
            "project_id": project_id,
            "project_name": project_name,
            "files": files_batch
        }
        
        logger.info(f"ğŸ“¤ ë°°ì¹˜ {batch_num}/{total_batches} ì—…ë¡œë“œ ì¤‘... ({len(files_batch)}ê°œ íŒŒì¼)")
        
        # í˜„ì¬ ì„œë²„ì˜ upload-batch ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
        upload_url = "http://localhost:8000/api/v1/upload-batch"
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                upload_url,
                json=upload_data,
                timeout=aiohttp.ClientTimeout(total=600)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"âœ… ë°°ì¹˜ {batch_num}/{total_batches} ì™„ë£Œ (ì„±ê³µë¥ : {result.get('success_rate', 0)}%)")
                    return result
                else:
                    error_text = await response.text()
                    logger.error(f"ë°°ì¹˜ {batch_num} ì—…ë¡œë“œ ì‹¤íŒ¨: {error_text}")
                    raise Exception(f"ë°°ì¹˜ {batch_num} ì—…ë¡œë“œ ì‹¤íŒ¨ (HTTP {response.status})")
    
    # ë°°ì¹˜ë“¤ì„ ë³‘ë ¬ë¡œ ì—…ë¡œë“œ
    upload_tasks = []
    for i in range(0, len(files_data), batch_size):
        batch = files_data[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        task = upload_batch(batch, batch_num)
        upload_tasks.append(task)
    
    # ëª¨ë“  ë°°ì¹˜ ì—…ë¡œë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
    batch_results = await asyncio.gather(*upload_tasks, return_exceptions=True)
    
    # ê²°ê³¼ ì§‘ê³„
    total_received = 0
    total_indexed = 0
    total_failed = 0
    tech_stacks = set()
    failed_batches = 0
    
    for i, result in enumerate(batch_results):
        if isinstance(result, Exception):
            logger.error(f"âŒ ë°°ì¹˜ {i+1} ì—…ë¡œë“œ ì‹¤íŒ¨: {result}")
            failed_batches += 1
            continue
        
        total_received += result.get('total_files_received', 0)
        total_indexed += result.get('indexed_files_count', 0)
        total_failed += result.get('failed_files_count', 0)
        tech_stacks.update(result.get('tech_stack', []))
    
    end_time = time.time()
    upload_time = end_time - start_time
    
    success_rate = (total_indexed / total_received * 100) if total_received > 0 else 0
    processing_speed = total_indexed / upload_time if upload_time > 0 else 0
    
    logger.info(f"ğŸ‰ ë„¤íŠ¸ì›Œí¬ ì—…ë¡œë“œ ì™„ë£Œ! ({upload_time:.2f}ì´ˆ)")
    logger.info(f"   ğŸ“¤ ì „ì†¡ëœ íŒŒì¼: {total_received}ê°œ")
    logger.info(f"   âœ… ì¸ë±ì‹±ëœ íŒŒì¼: {total_indexed}ê°œ") 
    logger.info(f"   âŒ ì‹¤íŒ¨í•œ íŒŒì¼: {total_failed}ê°œ")
    logger.info(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
    logger.info(f"   ğŸš€ ì²˜ë¦¬ ì†ë„: {processing_speed:.1f} íŒŒì¼/ì´ˆ")
    logger.info(f"   ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ: {', '.join(sorted(tech_stacks))}")
    
    return {
        "success": True,
        "project_id": project_id,
        "project_name": project_name,
        "project_path": str(project_path),
        "total_files_scanned": len(file_paths),
        "total_files_read": len(files_data),
        "total_files_received": total_received,
        "indexed_files_count": total_indexed,
        "failed_files_count": total_failed,
        "success_rate": round(success_rate, 1),
        "upload_time": round(upload_time, 2),
        "processing_speed": round(processing_speed, 1),
        "total_batches": total_batches,
        "failed_batches": failed_batches,
        "batch_size": batch_size,
        "max_workers": max_workers,
        "total_size_kb": round(total_size / 1024, 1),
        "tech_stack": sorted(list(tech_stacks)),
        "performance_metrics": {
            "files_per_second": round(processing_speed, 1),
            "kb_per_second": round((total_size / 1024) / upload_time, 1) if upload_time > 0 else 0,
            "batch_success_rate": round((total_batches - failed_batches) / total_batches * 100, 1) if total_batches > 0 else 0
        }
    }

if __name__ == "__main__":
    logger.info("FastMCP ì„œë²„ ì‹œì‘...")
    
    # ì„œë²„ ì‹¤í–‰ ì „ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    try:
        asyncio.run(initialize_services())
        logger.info("ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ, SSE ëª¨ë“œë¡œ MCP ì„œë²„ ì‹¤í–‰")
    except Exception as e:
        logger.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # SSE ë°©ì‹ìœ¼ë¡œ ì„œë²„ ì‹¤í–‰ (ê¸°ë³¸ í¬íŠ¸ 8000 ì‚¬ìš©)
    try:
        mcp.run(transport="sse")
    except KeyboardInterrupt:
        logger.info("ì„œë²„ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1) 