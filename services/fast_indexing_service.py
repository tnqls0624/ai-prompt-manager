import os
import logging
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
import asyncio
import aiofiles
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from models.prompt_models import ProjectContext, PromptHistory, PromptType
from services.vector_service import VectorService
from config import settings
import hashlib
from datetime import datetime
import time

logger = logging.getLogger(__name__)

class FastIndexingService:
    """ê³ ì„±ëŠ¥ ë³‘ë ¬ íŒŒì¼ ì¸ë±ì‹± ì„œë¹„ìŠ¤"""
    
    def __init__(self, vector_service: VectorService):
        self.vector_service = vector_service
        
        # ì„±ëŠ¥ ì„¤ì • (ëŒ€í­ ì¦ê°€)
        self.max_concurrent_files = 200  # 50 â†’ 200ìœ¼ë¡œ ì¦ê°€
        self.batch_size = 500  # 100 â†’ 500ìœ¼ë¡œ ì¦ê°€
        self.chunk_size = 2048  # 1024 â†’ 2048ë¡œ ì¦ê°€
        self.chunk_overlap = 200  # 100 â†’ 200ìœ¼ë¡œ ì¦ê°€
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        self.supported_extensions = {
            '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.cs',
            '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.clj',
            '.md', '.txt', '.rst', '.asciidoc', '.org',
            '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
            '.sql', '.sh', '.bash', '.zsh', '.fish', '.ps1', '.bat', '.cmd',
            '.html', '.css', '.scss', '.sass', '.less', '.vue', '.svelte',
            '.dart', '.r', '.hs', '.elm', '.xml', '.dockerfile', '.env'
        }
        
        # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ (í™•ì¥)
        self.ignore_directories = {
            'node_modules', 'bower_components', 'jspm_packages', 'typings',
            '__pycache__', '.pytest_cache', '.mypy_cache', 'venv', 'env', '.env',
            '.git', '.svn', '.hg', '.bzr', 'CVS',
            '.vscode', '.idea', '.vs', '.vscode-test',
            'dist', 'build', 'target', 'out', '.next', 'bin', 'obj',
            'Debug', 'Release', 'vendor', 'pkg', 'cache', 'tmp', 'temp',
            'coverage', 'logs', 'assets', 'public', 'static', 'chroma_db',
            '.tox', '.nox', 'htmlcov', '.coverage', '.nyc_output',
            'lib', 'libs', 'packages', 'deps', 'external'
        }
        
        # ë¬´ì‹œí•  íŒŒì¼
        self.ignore_files = {
            '.gitignore', '.dockerignore', '.env', '.env.local', '.env.production',
            'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', 'bun.lockb',
            'poetry.lock', 'Pipfile.lock', 'pdm.lock', 'requirements.txt',
            'composer.lock', 'Gemfile.lock', 'Cargo.lock', 'go.sum', 'go.mod',
            'mix.lock', 'pubspec.lock', '.DS_Store', 'Thumbs.db'
        }
        
        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹œ
        self.file_cache = {}  # íŒŒì¼ í•´ì‹œ ìºì‹œ
        
        # Thread/Process Pool (ì›Œì»¤ ì¦ê°€)
        self.thread_executor = ThreadPoolExecutor(max_workers=8)  # 4 â†’ 8ë¡œ ì¦ê°€
        
    async def index_project_files_fast(self, project_path: str, project_id: str) -> Dict[str, Any]:
        """ğŸš€ ê³ ì† ë³‘ë ¬ í”„ë¡œì íŠ¸ íŒŒì¼ ì¸ë±ì‹±"""
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ ê³ ì† ì¸ë±ì‹± ì‹œì‘: {project_path}")
            
            project_path = Path(project_path).resolve()
            
            if not project_path.exists():
                return {
                    "success": False,
                    "error": f"í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_path}"
                }
            
            # 1. í”„ë¡œì íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ë³‘ë ¬)
            project_info = await self._analyze_project_structure_fast(project_path)
            
            # 2. í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ì €ì¥
            project_context = ProjectContext(
                project_id=project_id,
                project_name=project_path.name,
                description=project_info.get('description', ''),
                tech_stack=project_info.get('tech_stack', []),
                file_patterns=project_info.get('file_patterns', [])
            )
            
            await self.vector_service.store_project_context(project_context)
            
            # 3. íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ (Thread Pool ì‚¬ìš©)
            logger.info("ğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            loop = asyncio.get_event_loop()
            file_paths = await loop.run_in_executor(
                self.thread_executor, 
                self._scan_files_fast, 
                project_path
            )
            
            logger.info(f"ğŸ“‹ ìŠ¤ìº” ì™„ë£Œ: {len(file_paths)}ê°œ íŒŒì¼ ë°œê²¬")
            
            # 4. ë³‘ë ¬ íŒŒì¼ ì²˜ë¦¬ (Semaphoreë¡œ ë™ì‹œì„± ì œì–´)
            semaphore = asyncio.Semaphore(self.max_concurrent_files)
            indexed_files = []
            failed_files = []
            
            # ì§„í–‰ë¥  ì¶”ì 
            total_files = len(file_paths)
            processed_count = 0
            
            async def process_file_with_semaphore(file_path):
                nonlocal processed_count
                async with semaphore:
                    try:
                        await self._index_single_file_fast(file_path, project_id)
                        processed_count += 1
                        
                        # ì§„í–‰ë¥  ë¡œê·¸ (10% ë‹¨ìœ„)
                        if processed_count % max(1, total_files // 10) == 0:
                            progress = (processed_count / total_files) * 100
                            logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({processed_count}/{total_files})")
                        
                        return str(file_path.relative_to(project_path))
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨ {file_path}: {e}")
                        failed_files.append(str(file_path))
                        return None
            
            # ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            logger.info("âš¡ ë³‘ë ¬ ì¸ë±ì‹± ì‹œì‘...")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´
            batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
            for i in range(0, len(file_paths), batch_size):
                batch_files = file_paths[i:i + batch_size]
                logger.info(f"ğŸ”„ ë°°ì¹˜ {i//batch_size + 1}/{(len(file_paths) + batch_size - 1)//batch_size} ì²˜ë¦¬ ì¤‘...")
                
                try:
                    batch_results = await asyncio.gather(
                        *[process_file_with_semaphore(fp) for fp in batch_files],
                        return_exceptions=True
                    )
                    
                    # ì„±ê³µí•œ íŒŒì¼ë“¤ë§Œ ìˆ˜ì§‘
                    for result in batch_results:
                        if result is not None and not isinstance(result, Exception):
                            indexed_files.append(result)
                            
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ê°œë³„ íŒŒì¼ë¡œ ì¬ì‹œë„
                    for fp in batch_files:
                        try:
                            result = await process_file_with_semaphore(fp)
                            if result is not None:
                                indexed_files.append(result)
                        except Exception as file_error:
                            logger.warning(f"ê°œë³„ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {fp}: {file_error}")
                            failed_files.append(str(fp))
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            result = {
                "success": True,
                "project_id": project_id,
                "project_name": project_context.project_name,
                "project_path": str(project_path),
                "total_files_found": len(file_paths),
                "indexed_files_count": len(indexed_files),
                "failed_files_count": len(failed_files),
                "processing_time_seconds": round(processing_time, 2),
                "files_per_second": round(len(indexed_files) / processing_time if processing_time > 0 else 0, 2),
                "tech_stack": project_context.tech_stack,
                "file_patterns": project_context.file_patterns,
                "indexed_files": indexed_files[:20],  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
                "failed_files": failed_files[:10] if failed_files else []
            }
            
            logger.info(f"âœ… ê³ ì† ì¸ë±ì‹± ì™„ë£Œ: {len(indexed_files)}ê°œ íŒŒì¼ ({processing_time:.2f}ì´ˆ, {result['files_per_second']:.1f} files/sec)")
            return result
            
        except Exception as e:
            logger.error(f"ê³ ì† ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "project_id": project_id
            }
    
    async def _analyze_project_structure_fast(self, project_path: Path) -> Dict[str, Any]:
        """ğŸ” ë³‘ë ¬ í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        tech_stack = set()
        file_patterns = set()
        description = ""
        
        # README íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        readme_files = ['README.md', 'README.txt', 'README.rst', 'README']
        
        async def read_readme(filename):
            readme_path = project_path / filename
            if readme_path.exists():
                try:
                    async with aiofiles.open(readme_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = await f.read()
                        lines = content.split('\n')
                        desc_lines = []
                        for line in lines[1:]:
                            line = line.strip()
                            if line and not line.startswith('#'):
                                desc_lines.append(line)
                            elif desc_lines:
                                break
                        return ' '.join(desc_lines)[:500]
                except Exception as e:
                    logger.warning(f"README íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {readme_path}: {e}")
            return None
        
        # README íŒŒì¼ë“¤ ë³‘ë ¬ ì½ê¸°
        readme_results = await asyncio.gather(
            *[read_readme(filename) for filename in readme_files],
            return_exceptions=True
        )
        
        # ì²« ë²ˆì§¸ ì„±ê³µí•œ README ì‚¬ìš©
        for result in readme_results:
            if result and not isinstance(result, Exception):
                description = result
                break
        
        # ì„¤ì • íŒŒì¼ë“¤ ë³‘ë ¬ í™•ì¸
        config_files = {
            'package.json': 'Node.js',
            'requirements.txt': 'Python',
            'Cargo.toml': 'Rust',
            'go.mod': 'Go',
            'pom.xml': 'Java/Maven',
            'build.gradle': 'Java/Gradle',
            'next.config.js': 'Next.js',
            'nuxt.config.js': 'Nuxt.js',
            'composer.json': 'PHP',
            'Gemfile': 'Ruby'
        }
        
        for config_file, tech in config_files.items():
            if (project_path / config_file).exists():
                tech_stack.add(tech)
        
        # íŒŒì¼ í™•ì¥ì ë¶„ì„ (ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥´ê²Œ)
        sample_files = list(self._scan_files_fast(project_path))[:100]  # ì²˜ìŒ 100ê°œë§Œ ìƒ˜í”Œë§
        
        for file_path in sample_files:
            ext = file_path.suffix.lower()
            file_patterns.add(f"*{ext}")
            
            # ê¸°ìˆ  ìŠ¤íƒ ì¶”ì •
            tech_mapping = {
                '.py': 'Python',
                '.js': 'JavaScript', '.jsx': 'JavaScript',
                '.ts': 'TypeScript', '.tsx': 'TypeScript',
                '.java': 'Java',
                '.cpp': 'C/C++', '.c': 'C/C++',
                '.cs': 'C#',
                '.go': 'Go',
                '.rs': 'Rust',
                '.php': 'PHP',
                '.rb': 'Ruby',
                '.swift': 'Swift',
                '.kt': 'Kotlin',
                '.vue': 'Vue.js',
                '.svelte': 'Svelte'
            }
            
            if ext in tech_mapping:
                tech_stack.add(tech_mapping[ext])
        
        return {
            'description': description,
            'tech_stack': sorted(list(tech_stack)),
            'file_patterns': sorted(list(file_patterns))
        }
    
    def _scan_files_fast(self, project_path: Path) -> List[Path]:
        """ğŸ“‚ ê³ ì† íŒŒì¼ ìŠ¤ìº” (Thread Poolì—ì„œ ì‹¤í–‰)"""
        files = []
        
        for root, dirs, filenames in os.walk(project_path):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ ì œê±°
            dirs[:] = [d for d in dirs if d not in self.ignore_directories]
            
            for filename in filenames:
                if filename in self.ignore_files:
                    continue
                    
                file_path = Path(root) / filename
                
                # íŒŒì¼ í¬ê¸° ì²´í¬
                try:
                    if file_path.stat().st_size > self.max_file_size:
                        continue
                except:
                    continue
                
                # ì§€ì›í•˜ëŠ” í™•ì¥ìë§Œ ì²˜ë¦¬
                if file_path.suffix.lower() in self.supported_extensions:
                    files.append(file_path)
        
        return files
    
    async def _index_single_file_fast(self, file_path: Path, project_id: str):
        """âš¡ ê³ ì† ë‹¨ì¼ íŒŒì¼ ì¸ë±ì‹±"""
        max_retries = 3
        retry_delay = 1  # ì´ˆ
        
        for attempt in range(max_retries):
            try:
                # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ìºì‹±ìš©)
                file_stat = file_path.stat()
                file_key = f"{file_path}_{file_stat.st_mtime}_{file_stat.st_size}"
                file_hash = hashlib.md5(file_key.encode()).hexdigest()
                
                # ìºì‹œ í™•ì¸ (ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ)
                if file_hash in self.file_cache:
                    return
                
                # ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°
                async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = await f.read()
                
                # ë¹ˆ íŒŒì¼ì´ë‚˜ ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
                if len(content.strip()) < 20:
                    return
                
                # ë” í° ì²­í¬ í¬ê¸°ë¡œ ì²˜ë¦¬ (API í˜¸ì¶œ ì¤„ì´ê¸°)
                if len(content) > self.chunk_size * 3:  # 6KB ì´ìƒì´ë©´ ì²­í‚¹
                    chunks = await self._chunk_file_content_fast(content, file_path)
                    
                    # ë°°ì¹˜ë¡œ ì²­í¬ë“¤ ì²˜ë¦¬
                    for i in range(0, len(chunks), self.batch_size):
                        batch_chunks = chunks[i:i + self.batch_size]
                        
                        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì¬ì‹œë„ ë¡œì§
                        for batch_attempt in range(max_retries):
                            try:
                                await asyncio.gather(*[
                                    self._store_file_chunk_fast(chunk, file_path, project_id, i + j)
                                    for j, chunk in enumerate(batch_chunks)
                                ])
                                break  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                            except Exception as batch_error:
                                if batch_attempt == max_retries - 1:
                                    raise batch_error
                                logger.warning(f"ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨ (ì‹œë„ {batch_attempt + 1}/{max_retries}): {batch_error}")
                                await asyncio.sleep(retry_delay * (batch_attempt + 1))
                else:
                    await self._store_file_chunk_fast(content, file_path, project_id, 0)
                
                # ìºì‹œì— ì¶”ê°€
                self.file_cache[file_hash] = file_path.name
                return  # ì„±ê³µí•˜ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"ê³ ì† íŒŒì¼ ì¸ë±ì‹± ìµœì¢… ì‹¤íŒ¨ {file_path}: {e}")
                    raise
                else:
                    logger.warning(f"ê³ ì† íŒŒì¼ ì¸ë±ì‹± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}) {file_path}: {e}")
                    await asyncio.sleep(retry_delay * (attempt + 1))  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    async def _chunk_file_content_fast(self, content: str, file_path: Path) -> List[str]:
        """âš¡ ê³ ì† íŒŒì¼ ì²­í‚¹"""
        if self.vector_service.text_splitter:
            try:
                # LangChain ì²­í‚¹ (ë” í° ì²­í¬ í¬ê¸°)
                self.vector_service.text_splitter.chunk_size = self.chunk_size
                self.vector_service.text_splitter.chunk_overlap = self.chunk_overlap  # ë” í° ì˜¤ë²„ë©
                chunks = self.vector_service.text_splitter.split_text(content)
                return chunks
            except Exception as e:
                logger.warning(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨ {file_path}: {e}")
        
        # ê¸°ë³¸ ì²­í‚¹ (ë” í° ì²­í¬)
        lines = content.split('\n')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for line in lines:
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size > self.chunk_size:
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_size = 0
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    async def _store_file_chunk_fast(self, content: str, file_path: Path, project_id: str, chunk_index: int):
        """âš¡ ê³ ì† íŒŒì¼ ì²­í¬ ì €ì¥"""
        # ê³ ìœ  ID ìƒì„±
        file_hash = hashlib.md5(f"{file_path}_{chunk_index}".encode()).hexdigest()
        chunk_id = f"{project_id}_file_{file_hash}"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "file_path": str(file_path),
            "file_name": file_path.name,
            "file_extension": file_path.suffix,
            "chunk_index": chunk_index,
            "file_type": self._detect_file_type(file_path),
            "is_file_content": True
        }
        
        # PromptHistory ê°ì²´ ìƒì„±
        prompt_history = PromptHistory(
            id=chunk_id,
            project_id=project_id,
            content=content,
            prompt_type=PromptType.SYSTEM_PROMPT,
            metadata=metadata,
            created_at=datetime.now()
        )
        
        await self.vector_service.store_prompt_history(prompt_history)
    
    def _detect_file_type(self, file_path: Path) -> str:
        """ğŸ“‹ íŒŒì¼ íƒ€ì… ê°ì§€"""
        ext = file_path.suffix.lower()
        
        code_extensions = {'.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.cs', '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala'}
        doc_extensions = {'.md', '.txt', '.rst', '.asciidoc'}
        config_extensions = {'.json', '.yaml', '.yml', '.toml', '.ini', '.cfg'}
        
        if ext in code_extensions:
            return "code"
        elif ext in doc_extensions:
            return "documentation"
        elif ext in config_extensions:
            return "configuration"
        else:
            return "other"
    
    async def batch_embed_texts(self, texts: List[str]) -> List[List[float]]:
        """ğŸ”¥ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ê³ ì„±ëŠ¥ ìµœì í™”)"""
        try:
            if not texts:
                return []
                
            if not self.vector_service.embeddings:
                # í´ë°±: ê°œë³„ ì„ë² ë”© ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
                semaphore = asyncio.Semaphore(20)  # ë™ì‹œì„± ì œì–´
                
                async def embed_with_semaphore(text):
                    async with semaphore:
                        return await self.vector_service._generate_embedding(text)
                
                return await asyncio.gather(*[
                    embed_with_semaphore(text) for text in texts
                ])
            
            # ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
            chunk_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜
            all_embeddings = []
            
            for i in range(0, len(texts), chunk_size):
                chunk_texts = texts[i:i + chunk_size]
                
                # DeepSeek ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ (ì´ë¯¸ ë‚´ë¶€ì ìœ¼ë¡œ ìµœì í™”ë¨)
                chunk_embeddings = await self.vector_service.embeddings.aembed_documents(chunk_texts)
                all_embeddings.extend(chunk_embeddings)
                
                # ë©”ëª¨ë¦¬ ì••ë°• ë°©ì§€ë¥¼ ìœ„í•œ ì‘ì€ ëŒ€ê¸°
                if i % (chunk_size * 5) == 0 and i > 0:
                    await asyncio.sleep(0.01)  # 10ms ëŒ€ê¸°
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # í´ë°±: ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
            return [[0.0] * 768] * len(texts)
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ğŸ“Š ì„±ëŠ¥ í†µê³„"""
        return {
            "max_concurrent_files": self.max_concurrent_files,
            "batch_size": self.batch_size,
            "chunk_size": self.chunk_size,
            "cache_size": len(self.file_cache),
            "supported_extensions": len(self.supported_extensions)
        }
    
    def __del__(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'thread_executor'):
                self.thread_executor.shutdown(wait=False, cancel_futures=True)
                logger.debug("ThreadPoolExecutor ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}") 