import logging
import os
from typing import List, Dict, Any, Optional
from models.prompt_models import PromptEnhanceRequest, PromptEnhanceResponse
from services.vector_service import VectorService, DeepSeekLLM
from services.advanced_analytics import AdvancedAnalyticsService
from config import settings

# ì„ íƒì  ì„í¬íŠ¸
try:
    from langchain_openai import OpenAI
except ImportError:
    try:
        from langchain.llms import OpenAI
    except ImportError:
        OpenAI = None

try:
    from langchain.prompts import PromptTemplate
except ImportError:
    PromptTemplate = None

try:
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError:
    try:
        from langchain.schema import HumanMessage, SystemMessage
    except ImportError:
        HumanMessage = None
        SystemMessage = None

logger = logging.getLogger(__name__)

class PromptEnhancementService:
    """í”„ë¡¬í”„íŠ¸ ê°œì„  ì„œë¹„ìŠ¤"""
    
    def __init__(self, vector_service: VectorService):
        self.vector_service = vector_service
        self.analytics_service = AdvancedAnalyticsService()
        self.llm = None
        
        # LLM ì´ˆê¸°í™” (ì„¤ì •ì— ë”°ë¼ OpenAI ë˜ëŠ” DeepSeek)
        embedding_model_type = os.getenv("EMBEDDING_MODEL_TYPE", settings.embedding_model_type)
        
        if embedding_model_type == "openai" and settings.openai_api_key and OpenAI:
            try:
                self.llm = OpenAI(
                    openai_api_key=settings.openai_api_key,
                    temperature=0.7,
                    max_tokens=2000
                )
            except Exception as e:
                logger.warning(f"OpenAI LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        elif embedding_model_type == "deepseek":
            try:
                deepseek_api_base = os.getenv("DEEPSEEK_API_BASE", settings.deepseek_api_base)
                deepseek_llm_model = os.getenv("DEEPSEEK_LLM_MODEL", settings.deepseek_llm_model)
                self.llm = DeepSeekLLM(
                    api_base=deepseek_api_base,
                    model_name=deepseek_llm_model,
                    temperature=0.7,
                    max_tokens=2000
                )
                logger.info(f"DeepSeek LLM ì´ˆê¸°í™” ì™„ë£Œ: {deepseek_api_base} (ëª¨ë¸: {deepseek_llm_model})")
            except Exception as e:
                logger.warning(f"DeepSeek LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.info("LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í¬ë§·í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.enhancement_template = None
        self.enhancement_chain = None
        self.prompt_formatter = StandardPromptFormatter()
        
        if PromptTemplate and self.llm:
            try:
                self.enhancement_template = PromptTemplate(
                    input_variables=["original_prompt", "project_context", "similar_prompts", "tech_stack", "file_context"],
                    template=self.prompt_formatter.get_enhancement_template()
                )
                
                # LangChain ìƒˆë¡œìš´ ë°©ì‹: prompt | llm (OpenAIì™€ DeepSeek ëª¨ë‘ ì§€ì›)
                try:
                    self.enhancement_chain = self.enhancement_template | self.llm
                    logger.info(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ì²´ì¸ ìƒì„± ì„±ê³µ: {type(self.llm).__name__}")
                except Exception as e:
                    logger.warning(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ì²´ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
                    self.enhancement_chain = None
            except Exception as e:
                logger.warning(f"LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def enhance_prompt(self, request: PromptEnhanceRequest) -> PromptEnhanceResponse:
        """í”„ë¡¬í”„íŠ¸ ê°œì„ """
        try:
            # 1. í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
            project_context = await self.vector_service.get_project_context(request.project_id)
            
            # 2. ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ ê²€ìƒ‰
            context_limit = request.context_limit or settings.max_context_length
            similar_prompts = await self.vector_service.search_similar_prompts(
                query=request.original_prompt,
                project_id=request.project_id,
                limit=context_limit
            )
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
            similar_prompts = [
                prompt for prompt in similar_prompts 
                if prompt.get('similarity', 0) >= settings.similarity_threshold
            ]
            
            # 3. ê´€ë ¨ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (í”„ë¡¬í”„íŠ¸ì™€ ê´€ë ¨ëœ ì½”ë“œ/ë¬¸ì„œ)
            file_context = await self.vector_service.search_similar_prompts(
                query=request.original_prompt + " code implementation documentation",
                project_id=request.project_id,
                limit=3
            )
            
            # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§
            file_context = [
                item for item in file_context 
                if item.get('metadata', {}).get('is_file_content', False)
            ]
            
            # 4. í‘œì¤€í™”ëœ í”„ë¡¬í”„íŠ¸ í¬ë§·ìœ¼ë¡œ ê°œì„ 
            enhanced_result = await self._enhance_with_standard_format(
                request.original_prompt,
                project_context,
                similar_prompts,
                file_context
            )
            
            # 5. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_score = self._calculate_confidence_score(
                project_context, similar_prompts, file_context, request.original_prompt
            )
            
            # 6. ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì •ë³´ êµ¬ì„±
            context_used = []
            # í”„ë¡¬í”„íŠ¸ íˆìŠ¤í† ë¦¬ ì¶”ê°€
            for prompt in similar_prompts:
                context_used.append(f"í”„ë¡¬í”„íŠ¸: {prompt['content'][:100]}...")
            # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            for file_item in file_context:
                file_path = file_item.get('metadata', {}).get('file_path', 'Unknown')
                context_used.append(f"íŒŒì¼: {file_path}")
            
            # 7. ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±
            suggestions = self._generate_suggestions(project_context, similar_prompts, file_context)
            
            return PromptEnhanceResponse(
                enhanced_prompt=enhanced_result.strip(),
                confidence_score=confidence_score,
                context_used=context_used,
                suggestions=suggestions
            )
            
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            return PromptEnhanceResponse(
                enhanced_prompt=request.original_prompt,
                confidence_score=0.0,
                context_used=[],
                suggestions=["í”„ë¡¬í”„íŠ¸ ê°œì„  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]
            )
    
    async def _enhance_with_standard_format(
        self,
        original_prompt: str,
        project_context: Optional[Dict[str, Any]],
        similar_prompts: List[Dict[str, Any]],
        file_context: List[Dict[str, Any]]
    ) -> str:
        """í‘œì¤€í™”ëœ í¬ë§·ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ """
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
        context_text = self._format_context(project_context)
        tech_stack_text = self._format_tech_stack(project_context)
        similar_prompts_text = self._format_similar_prompts(similar_prompts)
        file_context_text = self._format_file_context(file_context)
        
        # LLMì„ í†µí•œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
        if self.enhancement_chain:
            try:
                enhanced_result = await self.enhancement_chain.ainvoke({
                    "original_prompt": original_prompt,
                    "project_context": context_text,
                    "similar_prompts": similar_prompts_text,
                    "tech_stack": tech_stack_text,
                    "file_context": file_context_text
                })
                
                # LangChain ìƒˆë¡œìš´ ë°©ì‹ì—ì„œëŠ” AIMessage ê°ì²´ë‚˜ ë¬¸ìì—´ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
                if hasattr(enhanced_result, 'content'):
                    enhanced_result = enhanced_result.content
                elif not isinstance(enhanced_result, str):
                    enhanced_result = str(enhanced_result)
                    
                return enhanced_result
            except Exception as e:
                logger.warning(f"LLM í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
        elif self.llm and self.enhancement_template:
            try:
                # DeepSeek LLM ì§ì ‘ í˜¸ì¶œ
                prompt_text = self.enhancement_template.format(
                    original_prompt=original_prompt,
                    project_context=context_text,
                    similar_prompts=similar_prompts_text,
                    tech_stack=tech_stack_text,
                    file_context=file_context_text
                )
                enhanced_result = await self.llm.arun(prompt=prompt_text)
                return enhanced_result
            except Exception as e:
                logger.warning(f"DeepSeek LLM í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨: {e}")
        
        # LLMì´ ì‹¤íŒ¨í•˜ë©´ í‘œì¤€ í¬ë§·í„° ì‚¬ìš©
        return self.prompt_formatter.format_enhanced_prompt(
            original_prompt=original_prompt,
            project_context=project_context,
            similar_prompts=similar_prompts,
            file_context=file_context
        )
    
    def _format_context(self, project_context: Optional[Dict[str, Any]]) -> str:
        """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        if not project_context:
            return "í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        metadata = project_context.get("metadata", {})
        return f"""
í”„ë¡œì íŠ¸ëª…: {metadata.get('project_name', 'Unknown')}
ì„¤ëª…: {metadata.get('description', 'No description')}
"""

    def _format_tech_stack(self, project_context: Optional[Dict[str, Any]]) -> str:
        """ê¸°ìˆ  ìŠ¤íƒ í¬ë§·íŒ…"""
        if not project_context:
            return "ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        metadata = project_context.get("metadata", {})
        tech_stack = metadata.get('tech_stack', '').split(',')
        return f"ê¸°ìˆ  ìŠ¤íƒ: {', '.join(tech_stack)}" if tech_stack else "ê¸°ìˆ  ìŠ¤íƒ ì •ë³´ ì—†ìŒ"

    def _format_similar_prompts(self, similar_prompts: List[Dict[str, Any]]) -> str:
        """ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ë“¤ í¬ë§·íŒ…"""
        if not similar_prompts:
            return "ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for i, prompt in enumerate(similar_prompts[:3], 1):
            similarity = prompt.get('similarity', 0)
            content = prompt.get('content', '')[:200] + "..."
            formatted.append(f"{i}. (ìœ ì‚¬ë„: {similarity:.2f}) {content}")
        
        return "\n".join(formatted)

    def _format_file_context(self, file_context: List[Dict[str, Any]]) -> str:
        """íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        if not file_context:
            return "ê´€ë ¨ íŒŒì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for item in file_context:
            file_path = item.get('metadata', {}).get('file_path', 'Unknown')
            content = item.get('content', '')[:300] + "..."
            formatted.append(f"íŒŒì¼: {file_path}\në‚´ìš©: {content}")
        
        return "\n\n".join(formatted)

    def _calculate_confidence_score(
        self, 
        project_context: Optional[Dict[str, Any]], 
        similar_prompts: List[Dict[str, Any]], 
        file_context: List[Dict[str, Any]],
        original_prompt: str
    ) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ê¸°ë³¸ ì ìˆ˜ (í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¡´ì¬)
        if project_context:
            score += 0.3
        
        # ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ ì ìˆ˜
        if similar_prompts:
            avg_similarity = sum(p.get('similarity', 0) for p in similar_prompts) / len(similar_prompts)
            score += avg_similarity * 0.4
        
        # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜
        if file_context:
            score += 0.2
        
        # ê³ ê¸‰ ë¶„ì„ ì ìˆ˜
        advanced_score = self._calculate_advanced_score(original_prompt, similar_prompts, file_context)
        score += advanced_score * 0.1
        
        return min(score, 1.0)

    def _calculate_advanced_score(
        self,
        original_prompt: str,
        similar_prompts: List[Dict[str, Any]],
        file_context: List[Dict[str, Any]]
    ) -> float:
        """ê³ ê¸‰ ë¶„ì„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # í”„ë¡¬í”„íŠ¸ ë³µì¡ë„ ë¶„ì„
            complexity_score = self._analyze_prompt_complexity_score(original_prompt)
            
            # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ë¶„ì„
            relevance_score = self._calculate_context_relevance(original_prompt, similar_prompts, file_context)
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶„ì„
            quality_score = self._calculate_text_quality(original_prompt)
            
            return (complexity_score + relevance_score + quality_score) / 3
            
        except Exception as e:
            logger.warning(f"ê³ ê¸‰ ë¶„ì„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5

    def _analyze_prompt_complexity_score(self, prompt: str) -> float:
        """í”„ë¡¬í”„íŠ¸ ë³µì¡ë„ ë¶„ì„"""
        # ë‹¨ì–´ ìˆ˜, ë¬¸ì¥ ìˆ˜, ê¸°ìˆ  ìš©ì–´ ë“±ì„ ê³ ë ¤í•œ ë³µì¡ë„ ì ìˆ˜
        words = len(prompt.split())
        sentences = len([s for s in prompt.split('.') if s.strip()])
        
        # ê¸°ìˆ  ìš©ì–´ ì¹´ìš´íŠ¸
        tech_terms = ['function', 'class', 'method', 'variable', 'API', 'database', 'server', 'client']
        tech_count = sum(1 for term in tech_terms if term.lower() in prompt.lower())
        
        complexity = min((words / 50) + (sentences / 10) + (tech_count / 5), 1.0)
        return complexity

    def _calculate_context_relevance(
        self,
        prompt: str,
        similar_prompts: List[Dict[str, Any]],
        file_context: List[Dict[str, Any]]
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ê³„ì‚°"""
        if not similar_prompts and not file_context:
            return 0.0
        
        # ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ ê´€ë ¨ì„±
        prompt_relevance = 0.0
        if similar_prompts:
            prompt_relevance = sum(p.get('similarity', 0) for p in similar_prompts) / len(similar_prompts)
        
        # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        file_relevance = 0.0
        if file_context:
            prompt_words = set(prompt.lower().split())
            file_matches = 0
            for item in file_context:
                content = item.get('content', '').lower()
                file_words = set(content.split())
                overlap = len(prompt_words.intersection(file_words))
                file_matches += min(overlap / max(len(prompt_words), 1), 1.0)
            file_relevance = file_matches / len(file_context)
        
        return (prompt_relevance + file_relevance) / 2

    def _calculate_text_quality(self, prompt: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ê³„ì‚°"""
        # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ í’ˆì§ˆ ì§€í‘œ
        if not prompt.strip():
            return 0.0
        
        # ê¸¸ì´ ì ì ˆì„±
        length_score = min(len(prompt) / 100, 1.0) if len(prompt) > 10 else 0.3
        
        # ë¬¸ì¥ êµ¬ì¡° (ëŒ€ì†Œë¬¸ì, êµ¬ë‘ì  ë“±)
        structure_score = 0.5
        if prompt[0].isupper():
            structure_score += 0.2
        if any(punct in prompt for punct in '.!?'):
            structure_score += 0.2
        if len(prompt.split()) > 3:
            structure_score += 0.1
        
        return min((length_score + structure_score) / 2, 1.0)

    def _generate_suggestions(
        self, 
        project_context: Optional[Dict[str, Any]], 
        similar_prompts: List[Dict[str, Any]],
        file_context: List[Dict[str, Any]]
    ) -> List[str]:
        """ê°œì„  ì œì•ˆì‚¬í•­ ìƒì„±"""
        suggestions = []
        
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì œì•ˆ
        if project_context:
            metadata = project_context.get("metadata", {})
            tech_stack = metadata.get("tech_stack", "").split(",")
            if tech_stack:
                suggestions.append(f"í”„ë¡œì íŠ¸ì˜ ê¸°ìˆ  ìŠ¤íƒ({', '.join(tech_stack)})ì„ ê³ ë ¤í•˜ì—¬ ë” êµ¬ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.")
        
        # ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì œì•ˆ
        if similar_prompts:
            suggestions.append("ê³¼ê±° ìœ ì‚¬í•œ í”„ë¡¬í”„íŠ¸ë“¤ì˜ íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ë” ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì œì•ˆ
        if file_context:
            suggestions.append("ê´€ë ¨ ì½”ë“œë² ì´ìŠ¤ì˜ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ê³ ë ¤í•˜ì—¬ ê¸°ì¡´ ì•„í‚¤í…ì²˜ì™€ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.")
        
        # ì¼ë°˜ì ì¸ ì œì•ˆ
        suggestions.extend([
            "êµ¬ì²´ì ì¸ ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ í•˜ì„¸ìš”.",
            "ì˜ˆìƒë˜ëŠ” ì—ëŸ¬ ìƒí™©ê³¼ ì²˜ë¦¬ ë°©ë²•ì„ ëª…ì‹œí•˜ì„¸ìš”.",
            "ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì´ë‚˜ ì œì•½ì‚¬í•­ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”."
        ])
        
        return suggestions[:5]  # ìµœëŒ€ 5ê°œ ì œì•ˆ

    def _simple_prompt_enhancement(self, original_prompt: str, context: str, tech_stack: str, similar_prompts: str, file_context: str) -> str:
        """ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ ê°œì„  (LLM ì—†ì´)"""
        return self.prompt_formatter.format_enhanced_prompt(
            original_prompt=original_prompt,
            project_context={"metadata": {"description": context}},
            similar_prompts=[{"content": similar_prompts}],
            file_context=[{"content": file_context}]
        )

    async def analyze_prompt_patterns(self, project_id: str) -> Dict[str, Any]:
        """í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ë¶„ì„"""
        try:
            return await self.analytics_service.analyze_prompt_patterns(project_id)
        except Exception as e:
            logger.error(f"í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}


class StandardPromptFormatter:
    """í‘œì¤€í™”ëœ í”„ë¡¬í”„íŠ¸ í¬ë§·í„°"""
    
    def get_enhancement_template(self) -> str:
        """ê°œì„ ìš© í…œí”Œë¦¿ ë°˜í™˜"""
        return """
# ğŸš€ AI í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œìŠ¤í…œ

## ğŸ“‹ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸
**í”„ë¡œì íŠ¸ ì •ë³´:**
{project_context}

**ê¸°ìˆ  ìŠ¤íƒ:**
{tech_stack}

**ê´€ë ¨ ì½”ë“œ/ë¬¸ì„œ:**
{file_context}

**ìœ ì‚¬í•œ ê³¼ê±° í”„ë¡¬í”„íŠ¸:**
{similar_prompts}

## ğŸ¯ ê°œì„  ëŒ€ìƒ í”„ë¡¬í”„íŠ¸
```
{original_prompt}
```

## ğŸ’¡ ê°œì„  ì§€ì¹¨
1. **ëª…í™•ì„±**: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
2. **ì¼ê´€ì„±**: ê¸°ì¡´ ì½”ë“œë² ì´ìŠ¤ì™€ ì•„í‚¤í…ì²˜ íŒ¨í„´ ì¤€ìˆ˜
3. **ì™„ì „ì„±**: í•„ìš”í•œ ì •ë³´ì™€ ì œì•½ì‚¬í•­ ëª¨ë‘ í¬í•¨
4. **ì‹¤ìš©ì„±**: ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ ëª…í™•í•œ ì§€ì¹¨ ì œê³µ

## âœ¨ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
"""
    
    def format_enhanced_prompt(
        self,
        original_prompt: str,
        project_context: Optional[Dict[str, Any]] = None,
        similar_prompts: Optional[List[Dict[str, Any]]] = None,
        file_context: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """í‘œì¤€ í¬ë§·ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ """
        
        # í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        project_name = "Unknown Project"
        tech_stack = []
        if project_context:
            metadata = project_context.get("metadata", {})
            project_name = metadata.get("project_name", "Unknown Project")
            tech_stack = metadata.get("tech_stack", "").split(",")
        
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        enhanced_prompt = f"""
# ğŸ¯ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸

## ğŸ“‹ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
- **í”„ë¡œì íŠ¸**: {project_name}
- **ê¸°ìˆ  ìŠ¤íƒ**: {", ".join(tech_stack) if tech_stack else "ë¯¸ì§€ì •"}

## ğŸš€ ìš”êµ¬ì‚¬í•­
{original_prompt}

## ğŸ”§ êµ¬í˜„ ì§€ì¹¨
1. **ì•„í‚¤í…ì²˜**: ê¸°ì¡´ í”„ë¡œì íŠ¸ êµ¬ì¡°ì™€ ì¼ê´€ì„± ìœ ì§€
2. **ì½”ë“œ í’ˆì§ˆ**: í´ë¦° ì½”ë“œ ì›ì¹™ ì¤€ìˆ˜
3. **í…ŒìŠ¤íŠ¸**: ì ì ˆí•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ í¬í•¨
4. **ë¬¸ì„œí™”**: í•„ìš”í•œ ì£¼ì„ê³¼ ë¬¸ì„œ ì‘ì„±

## ğŸ“ ì¶”ê°€ ê³ ë ¤ì‚¬í•­
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì˜ˆì™¸ ìƒí™© ëŒ€ì‘
- ì„±ëŠ¥ ìµœì í™” ë°©ì•ˆ
- ë³´ì•ˆ ë° ê²€ì¦ ë¡œì§
- ìœ ì§€ë³´ìˆ˜ì„± í™•ë³´
"""
        
        # ìœ ì‚¬ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ì ìš©
        if similar_prompts:
            enhanced_prompt += "\n\n## ğŸ“š ì°¸ê³  íŒ¨í„´\n"
            for i, prompt in enumerate(similar_prompts[:2], 1):
                content = prompt.get("content", "")[:150] + "..."
                enhanced_prompt += f"{i}. {content}\n"
        
        # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ ì ìš©
        if file_context:
            enhanced_prompt += "\n\n## ğŸ” ê´€ë ¨ ì½”ë“œë² ì´ìŠ¤\n"
            for item in file_context[:2]:
                file_path = item.get("metadata", {}).get("file_path", "Unknown")
                enhanced_prompt += f"- ì°¸ê³  íŒŒì¼: {file_path}\n"
        
        return enhanced_prompt.strip() 