#!/usr/bin/env python3
"""
ğŸš€ ê³ ì„±ëŠ¥ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬ì™€ ë³‘ë ¬ ì—…ë¡œë“œë¥¼ í†µí•œ ë¹ ë¥¸ ì¸ë±ì‹±
"""

import asyncio
import aiohttp
import aiofiles
import json
import os
import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import math
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ìë“¤ (í™•ì¥ë¨)
SUPPORTED_EXTENSIONS = {
    '.py', '.js', '.jsx', '.ts', '.tsx', '.java', '.cpp', '.c', '.cs',
    '.go', '.rs', '.php', '.rb', '.swift', '.kt', '.scala', '.clj',
    '.md', '.txt', '.rst', '.asciidoc', '.org',
    '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
    '.sql', '.sh', '.bash', '.zsh', '.fish', '.ps1', '.bat', '.cmd',
    '.html', '.css', '.scss', '.sass', '.less', '.vue', '.svelte',
    '.dart', '.r', '.hs', '.elm', '.xml', '.dockerfile', '.env'
}

# ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ë“¤ (í™•ì¥ë¨)
IGNORE_DIRECTORIES = {
    'node_modules', 'bower_components', 'jspm_packages', 'typings',
    '__pycache__', '.pytest_cache', '.mypy_cache', 'venv', 'env', '.env',
    '.git', '.svn', '.hg', '.bzr', 'CVS',
    '.vscode', '.idea', '.vs', '.vscode-test',
    'dist', 'build', 'target', 'out', '.next', 'bin', 'obj',
    'Debug', 'Release', 'vendor', 'pkg', 'cache', 'tmp', 'temp',
    'coverage', 'logs', 'assets', 'public', 'static', 'chroma_db',
    '.tox', '.nox', 'htmlcov', '.coverage', '.nyc_output',
    'lib', 'libs', 'packages', 'deps', 'external'
}

# ë¬´ì‹œí•  íŒŒì¼ë“¤ (í™•ì¥ë¨)
IGNORE_FILES = {
    '.gitignore', '.dockerignore', '.env', '.env.local', '.env.production',
    'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', 'bun.lockb',
    'poetry.lock', 'Pipfile.lock', 'pdm.lock', 'requirements.txt',
    'composer.lock', 'Gemfile.lock', 'Cargo.lock', 'go.sum', 'go.mod',
    'mix.lock', 'pubspec.lock', '.DS_Store', 'Thumbs.db'
}

class FastProjectUploader:
    """ê³ ì„±ëŠ¥ í”„ë¡œì íŠ¸ ì—…ë¡œë”"""
    
    def __init__(self, server_url: str = "http://localhost:8000"):
        self.server_url = server_url
        self.session = None
        self.max_file_size = 50 * 1024 * 1024  # 50MB
        self.max_workers = 50  # ë³‘ë ¬ íŒŒì¼ ì½ê¸° ì›Œì»¤ ìˆ˜ ì¦ê°€
        self.batch_size = 300  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        
    async def __aenter__(self):
        # ì—°ê²° í’€ ìµœì í™”
        connector = aiohttp.TCPConnector(
            limit=200,  # ì „ì²´ ì—°ê²° í’€ í¬ê¸°
            limit_per_host=100,  # í˜¸ìŠ¤íŠ¸ë‹¹ ì—°ê²° ìˆ˜
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=60,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(
            total=600,  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
            connect=30,
            sock_read=120
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def upload_project(
        self,
        project_path: str,
        project_id: str = "default",
        project_name: str = None
    ) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ë° ì¸ë±ì‹±"""
        start_time = time.time()
        
        logger.info(f"ğŸš€ ê³ ì„±ëŠ¥ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ì‹œì‘: {project_path}")
        
        project_path = Path(project_path).resolve()
        
        if not project_path.exists():
            return {
                "success": False,
                "error": f"í”„ë¡œì íŠ¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_path}"
            }
        
        if not project_name:
            project_name = project_path.name
        
        # 1. íŒŒì¼ ìŠ¤ìº” (Thread Pool ì‚¬ìš©)
        logger.info("ğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        with ThreadPoolExecutor(max_workers=4) as executor:
            loop = asyncio.get_event_loop()
            file_paths = await loop.run_in_executor(
                executor, self._scan_files, project_path
            )
        
        logger.info(f"ğŸ“‹ {len(file_paths)}ê°œ íŒŒì¼ ë°œê²¬")
        
        if not file_paths:
            return {
                "success": False,
                "error": "ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # 2. ë³‘ë ¬ íŒŒì¼ ì½ê¸°
        logger.info("ğŸ“– íŒŒì¼ ì½ê¸° ì‹œì‘...")
        files_data = await self._read_files_parallel(file_paths, project_path)
        
        logger.info(f"âœ… {len(files_data)}ê°œ íŒŒì¼ ì½ê¸° ì™„ë£Œ")
        
        if not files_data:
            return {
                "success": False,
                "error": "ì½ì„ ìˆ˜ ìˆëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # 3. ë°°ì¹˜ ì—…ë¡œë“œ
        logger.info("ğŸ“¤ ë°°ì¹˜ ì—…ë¡œë“œ ì‹œì‘...")
        result = await self._upload_batches(files_data, project_id, project_name)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        result["total_time"] = round(total_time, 2)
        result["files_per_second"] = round(len(files_data) / total_time, 1) if total_time > 0 else 0
        
        logger.info(f"ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ! ({total_time:.2f}ì´ˆ)")
        logger.info(f"   ğŸ“Š ì²˜ë¦¬ ì†ë„: {result['files_per_second']:.1f} íŒŒì¼/ì´ˆ")
        
        return result
    
    def _scan_files(self, project_path: Path) -> List[Path]:
        """íŒŒì¼ ìŠ¤ìº” (ë™ê¸° í•¨ìˆ˜)"""
        file_paths = []
        
        for root, dirs, files in os.walk(project_path):
            # ë¬´ì‹œí•  ë””ë ‰í† ë¦¬ ì œê±°
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRECTORIES]
            
            for file in files:
                if file in IGNORE_FILES or file.startswith('.'):
                    continue
                
                file_path = Path(root) / file
                
                # ì§€ì›í•˜ëŠ” í™•ì¥ìë§Œ ì²˜ë¦¬
                if file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                    # íŒŒì¼ í¬ê¸° ì²´í¬
                    try:
                        if file_path.stat().st_size <= self.max_file_size:
                            file_paths.append(file_path)
                    except OSError:
                        continue
        
        return file_paths
    
    async def _read_files_parallel(
        self, 
        file_paths: List[Path], 
        project_path: Path
    ) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ íŒŒì¼ ì½ê¸°"""
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def read_file_with_semaphore(file_path: Path):
            async with semaphore:
                return await self._read_file(file_path, project_path)
        
        # ëª¨ë“  íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì½ê¸°
        results = await asyncio.gather(
            *[read_file_with_semaphore(fp) for fp in file_paths],
            return_exceptions=True
        )
        
        # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§
        files_data = []
        for result in results:
            if isinstance(result, dict) and result is not None:
                files_data.append(result)
        
        return files_data
    
    async def _read_file(self, file_path: Path, project_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ì½ê¸°"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = await f.read()
            
            # ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
            if len(content.strip()) < 10:
                return None
            
            relative_path = file_path.relative_to(project_path)
            return {
                "path": str(relative_path),
                "content": content,
                "size": len(content)
            }
            
        except Exception as e:
            logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    async def _upload_batches(
        self,
        files_data: List[Dict[str, Any]],
        project_id: str,
        project_name: str
    ) -> Dict[str, Any]:
        """ë°°ì¹˜ ì—…ë¡œë“œ"""
        total_batches = math.ceil(len(files_data) / self.batch_size)
        logger.info(f"ğŸ“¦ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì—…ë¡œë“œ...")
        
        # ë°°ì¹˜ ì—…ë¡œë“œ íƒœìŠ¤í¬ ìƒì„±
        upload_tasks = []
        for i in range(0, len(files_data), self.batch_size):
            batch = files_data[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            
            task = self._upload_single_batch(batch, batch_num, total_batches, project_id, project_name)
            upload_tasks.append(task)
        
        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì—…ë¡œë“œ
        batch_results = await asyncio.gather(*upload_tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì§‘ê³„
        return self._aggregate_results(batch_results, files_data)
    
    async def _upload_single_batch(
        self,
        batch: List[Dict[str, Any]],
        batch_num: int,
        total_batches: int,
        project_id: str,
        project_name: str
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë°°ì¹˜ ì—…ë¡œë“œ"""
        upload_data = {
            "project_id": project_id,
            "project_name": project_name,
            "files": batch
        }
        
        logger.info(f"ğŸ“¤ ë°°ì¹˜ {batch_num}/{total_batches} ì—…ë¡œë“œ... ({len(batch)}ê°œ íŒŒì¼)")
        
        upload_url = f"{self.server_url}/api/v1/upload-batch"
        
        try:
            async with self.session.post(upload_url, json=upload_data) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ (ì„±ê³µë¥ : {result.get('success_rate', 0)}%)")
                    return result
                else:
                    error_text = await response.text()
                    logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {error_text}")
                    raise Exception(f"HTTP {response.status}: {error_text}")
                    
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}
    
    def _aggregate_results(
        self, 
        batch_results: List[Any], 
        files_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ì§‘ê³„"""
        total_received = 0
        total_indexed = 0
        total_failed = 0
        failed_batches = 0
        tech_stacks = set()
        
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                logger.error(f"âŒ ë°°ì¹˜ {i+1} ì‹¤íŒ¨: {result}")
                failed_batches += 1
                continue
            
            if not result.get("success", False):
                failed_batches += 1
                continue
            
            total_received += result.get('total_files_received', 0)
            total_indexed += result.get('indexed_files_count', 0)
            total_failed += result.get('failed_files_count', 0)
            tech_stacks.update(result.get('tech_stack', []))
        
        success_rate = (total_indexed / total_received * 100) if total_received > 0 else 0
        
        return {
            "success": True,
            "total_files_scanned": len(files_data),
            "total_files_received": total_received,
            "indexed_files_count": total_indexed,
            "failed_files_count": total_failed,
            "success_rate": round(success_rate, 1),
            "total_batches": len(batch_results),
            "failed_batches": failed_batches,
            "batch_success_rate": round((len(batch_results) - failed_batches) / len(batch_results) * 100, 1) if batch_results else 0,
            "tech_stack": sorted(list(tech_stacks))
        }

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê³ ì„±ëŠ¥ í”„ë¡œì íŠ¸ ì—…ë¡œë“œ")
    parser.add_argument("project_path", help="ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê²½ë¡œ")
    parser.add_argument("--project-id", default="default", help="í”„ë¡œì íŠ¸ ID")
    parser.add_argument("--project-name", help="í”„ë¡œì íŠ¸ ì´ë¦„")
    parser.add_argument("--server-url", default="http://localhost:8000", help="ì„œë²„ URL")
    
    args = parser.parse_args()
    
    async with FastProjectUploader(args.server_url) as uploader:
        result = await uploader.upload_project(
            args.project_path,
            args.project_id,
            args.project_name
        )
        
        if result["success"]:
            print(f"\nğŸ‰ ì—…ë¡œë“œ ì„±ê³µ!")
            print(f"   ğŸ“Š ì´ íŒŒì¼: {result['total_files_scanned']}ê°œ")
            print(f"   âœ… ì¸ë±ì‹±: {result['indexed_files_count']}ê°œ")
            print(f"   ğŸ“ˆ ì„±ê³µë¥ : {result['success_rate']:.1f}%")
            print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {result['total_time']:.2f}ì´ˆ")
            print(f"   ğŸš€ ì²˜ë¦¬ì†ë„: {result['files_per_second']:.1f} íŒŒì¼/ì´ˆ")
            print(f"   ğŸ”§ ê¸°ìˆ ìŠ¤íƒ: {', '.join(result['tech_stack'])}")
        else:
            print(f"\nâŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 